# 通过可扩展查找实现的条件记忆：大型语言模型稀疏性的新维度

# 摘要

虽然混合专家模型 (Mixture-of-Experts, MoE) 通过条件计算扩展了模型容量，但 Transformer 缺乏用于知识查找的原生原语 (primitive)，迫使它们通过计算低效地模拟检索。为了解决这个问题，我们引入**条件记忆**作为一种互补的稀疏轴 (sparsity axis)，并通过 **Engram** 来实例化，该模块将经典的 -gram 嵌入现代化以实现  的查找。通过制定**稀疏分配** (Sparsity Allocation) 问题，我们发现了一个 U 型缩放定律，优化了神经计算 (MoE) 和静态记忆 (Engram) 之间的权衡。在该定律的指导下，我们将 Engram 扩展到 270 亿参数，在严格的等参数 (iso-parameter) 和等 FLOPs 的 MoE 基线上取得了优越的性能。最值得注意的是，虽然预期记忆模块有助于知识检索（例如，MMLU +3.4; CMMLU +4.0），但我们观察到在通用推理（例如，BBH +5.0; ARC-Challenge +3.7）和代码/数学领域（HumanEval +3.0; MATH +2.4）有更大的收益。机制分析表明，Engram 将骨干网络的早期层从静态重构中解放出来，有效地加深了用于复杂推理的网络深度。此外，通过将局部依赖关系委托给查找操作，它释放了注意力容量以关注全局上下文，从而大幅提升了长上下文检索能力（例如，Multi-Query NIAH: 84.2 → 97.0）。最后，Engram 建立了基础设施感知的效率：其确定性寻址支持从主机内存进行运行时预取，产生的开销几乎可以忽略不计。我们设想条件记忆将成为下一代稀疏模型不可或缺的建模原语。代码发布于：[https://github.com/deepseek-ai/Engram](https://github.com/deepseek-ai/Engram)

# 1. 引言

稀疏性是智能系统反复出现的设计原则，从生物神经回路 (Lennie, 2003; Olshausen and Field, 1997) 到现代大型语言模型 (LLMs)。目前，这一原则主要通过混合专家模型 (MoE) (Dai et al., 2024; Shazeer et al., 2017) 来实现，该模型通过条件计算来扩展容量。由于 MoE 能够大幅增加模型规模而不成比例地增加计算量，它已成为前沿模型的事实标准 (Comanici et al., 2025; Guo et al., 2025; Team et al., 2025)。

尽管这种条件计算范式取得了成功，但语言信号内在的异质性表明结构优化仍有很大空间。具体来说，语言建模包含两个性质截然不同的子任务：**组合推理**和**知识检索**。前者需要深度的、动态的计算，而文本的很大一部分——如命名实体和公式化模式——是局部的、静态的和高度刻板的 (Constant et al., 2017; Erman, 2000)。经典的 -gram 模型 (Brants et al., 2007; Liu et al., 2024b; Nguyen, 2024) 在捕捉此类局部依赖关系方面的有效性意味着，这些规律自然可以表示为计算成本低廉的查找操作。由于标准 Transformer (Vaswani et al., 2017) 缺乏原生的知识查找原语，当前的 LLM 被迫通过计算来模拟检索。例如，解析一个常见的多 Token 实体需要消耗多个早期层的注意力和前馈网络 (Ghandeharioun et al., 2024; Jin et al., 2025) (见表 3)。这个过程本质上相当于在运行时昂贵地重构一个静态查找表，将宝贵的序列深度浪费在微不足道的操作上，而这些深度本可以分配给更高级别的推理。

为了使模型架构与这种语言二元性相一致，我们提倡一种互补的稀疏轴：**条件记忆**。条件计算稀疏地激活参数来处理动态逻辑 (Bengio et al., 2013; Shazeer et al., 2017)，而条件记忆则依赖于稀疏查找操作来检索固定知识的静态嵌入。作为对该范式的初步探索，我们重新审视 -gram 嵌入 (Bojanowski et al., 2017) 作为一个典型的实例化：局部上下文作为一个键，通过常数时间  的查找来索引一个巨大的嵌入表 (Huang et al., 2025a; Pagnoni et al., 2025; Tito Svenstrup et al., 2017; Yu et al., 2025)。我们的调查显示，也许令人惊讶的是，这种静态检索机制可以作为现代 MoE 架构的理想补充——但前提是设计得当。在本文中，我们提出了 **Engram**，这是一个基于经典 -gram 结构但配备了现代适应性（如 Tokenizer 压缩、多头哈希、上下文感知门控和多分支集成，详见第 2 节）的条件记忆模块。

为了量化这两个原语之间的协同作用，我们制定了**稀疏分配**问题：给定固定的总参数预算，如何在 MoE 专家和 Engram 记忆之间分配容量？我们的实验发现了一个明显的 **U 型缩放定律**，表明即使是简单的查找机制，当被视为一流的建模原语时，也能充当神经计算的重要补充。在这个分配定律的指导下，我们将 Engram 扩展为一个 270 亿参数的模型。与严格等参数和等 FLOPs 的 MoE 基线相比，Engram-27B 在不同领域都实现了卓越的效率。至关重要的是，收益并不仅限于知识密集型任务（例如，MMLU: +3.4; CMMLU: +4.0; MMLU-Pro: +1.8），在这些任务中记忆容量直观上是有益的；我们在通用推理（例如，BBH: +5.0; ARC-Challenge: +3.7; DROP: +3.3）和代码/数学领域（例如，HumanEval: +3.0; MATH: +2.4; GSM8K: +2.2）观察到了更显著的提升。

通过 LogitLens (nostalgebraist, 2020) 和 CKA (Hendrycks et al., 2021a) 进行的机制分析揭示了这些收益的来源：Engram 将骨干网络从早期层的静态知识重构中解脱出来，从而增加了可用于复杂推理的有效深度。此外，通过将局部依赖关系委托给查找，Engram 释放了注意力容量以专注于全局上下文，从而在长上下文场景中实现了卓越的性能——在 LongPPL (Fang et al.) 和 RULER (Hsieh et al.) 上大幅优于基线（例如，Multi-Query NIAH: 97.0 vs. 84.2; Variable Tracking: 89.0 vs. 77.0）。

最后，我们将**基础设施感知效率**确立为首要原则。与 MoE 的动态路由不同，Engram 采用确定性 ID 来实现运行时预取，将通信与计算重叠。实证结果表明，将 1000 亿参数的表卸载到主机内存产生的开销可以忽略不计 ()。这表明 Engram 有效地绕过了 GPU 内存限制，促进了激进的参数扩展。

图 1 | Engram 架构。该模块通过检索静态 -gram 记忆并通过上下文感知门控将其与动态隐藏状态融合来增强骨干网络。该模块仅应用于特定层，以将记忆与计算解耦，保持标准输入嵌入和非嵌入模块（un-embedding module）不变。

# 2. 架构

# 2.1. 概述

如图 1 所示，Engram 是一个条件记忆模块，旨在通过在结构上分离静态模式存储与动态计算来增强 Transformer 骨干网络。形式上，给定输入序列  和第  层的隐藏状态 ，模块在两个功能阶段处理每个位置 ：**检索**和**融合**。首先，如第 2.2 节详述，我们提取并压缩后缀 -grams，通过哈希确定性地检索静态嵌入向量。随后，在第 2.3 节中，这些检索到的嵌入由当前隐藏状态动态调节，并通过轻量级卷积进行细化。最后，我们在第 2.4 节讨论与多分支架构的集成，并在第 2.5 节讨论系统级设计。

# 2.2. 基于哈希 -grams 的稀疏检索

第一阶段将局部上下文映射到静态记忆条目，涉及 Tokenizer 压缩并通过确定性哈希检索嵌入。

**Tokenizer 压缩** 虽然 -gram 模型通常直接在 Tokenizer 输出上操作，但标准的子词 Tokenizer 优先考虑无损重构，经常为语义等价的术语分配不相交的 ID（例如，Apple 与 apple）(Kudo and Richardson, 2018; Li et al., 2023b)。为了最大化语义密度，我们实现了一个词汇投影层。具体来说，我们预先计算一个满射函数 ，将原始 Token ID 折叠为基于标准化文本等价性（使用 NFKC (Whistler, 2025)，小写化等）的规范标识符。在实践中，此过程使 128k Tokenizer 的有效词汇量减少了 （见附录 C）。形式上，对于位置  的 Token，我们将其原始 ID  映射到规范 ID  以形成后缀 -gram 。

**多头哈希** 直接参数化所有可能 -grams 的组合空间是难以处理的。遵循 Tito Svenstrup et al. (2017) 的方法，我们采用基于哈希的方法。为了减轻冲突，我们为每个 -gram 阶数  使用  个不同的哈希头。每个头  通过确定性函数  将压缩的上下文映射到嵌入表 （大小为素数 ）内的索引：

在实践中， 被实现为轻量级的乘法-XOR 哈希。我们通过连接所有检索到的嵌入来构建最终的记忆向量 ：

# 2.3. 上下文感知门控

检索到的嵌入  充当上下文无关的先验。然而，由于是静态的，它们本质上缺乏上下文适应性，并且可能因哈希冲突或多义词而受到噪声的影响 (Haber and Poesio, 2024)。为了增强表达能力并解决这种歧义，我们采用受 Attention (Bahdanau et al., 2015; Vaswani et al., 2017) 启发的上下文感知门控机制。具体来说，我们利用当前隐藏状态  ——它已通过前面的注意力层聚合了全局上下文——作为动态 Query，而检索到的记忆  同时作为 Key 和 Value 投影的源：

其中  是可学习的投影矩阵。为了确保梯度稳定性 (Dehghani et al., 2023)，我们在计算标量门  之前对 Query 和 Key 应用 RMSNorm (Zhang and Sennrich, 2019)：

门控输出定义为 。这种设计强制执行语义对齐：如果检索到的记忆  与当前上下文  矛盾，门  倾向于零，从而有效地抑制噪声。

最后，为了扩大感受野并增强模型的非线性，我们引入了一个短的、深度方向的因果卷积 (Gu et al., 2022; Peng et al., 2023)。设  表示门控值的序列。使用核大小 （设为 4），膨胀率 （设为最大 -gram 阶数）和 SiLU 激活 (Elfwing et al., 2018)，最终输出  计算如下：

Engram 模块通过残差连接集成到骨干网络中：，随后是标准的 Attention 和 MoE。至关重要的是，Engram 并不应用于每一层；其具体位置由第 2.5 节详述的系统级延迟约束控制。

(a) 训练时的 Engram

(b) 推理时的 Engram

图 2 | Engram 的系统实现。(a) 训练阶段：巨大的嵌入表被分片到可用的 GPU 上。使用 All-to-All 通信原语在设备间检索活动的嵌入行。(b) 推理阶段：Engram 表被卸载到主机内存。通过利用确定性检索逻辑，主机异步预取并传输嵌入，将通信与之前的 Transformer 块的设备上计算重叠。

# 2.4. 与多分支架构的集成

在这项工作中，我们采用先进的多分支架构作为我们的默认骨干网络，而不是标准的单流连接 (He et al., 2016)，因为其具有卓越的建模能力 (Larsson et al., 2017; Szegedy et al., 2015; Xie et al., 2025; Zhu et al., 2025)。这种架构的一个定义特征是将残差流扩展为  个并行分支，其中信息流由可学习的连接权重调节。

虽然 Engram 模块本质上与拓扑无关，但将其适应于这种多分支框架需要进行结构优化，以平衡效率和表达能力。具体来说，我们实施了一种**参数共享策略**：单个稀疏嵌入表和 Value 投影矩阵  在所有  个分支之间共享，而使用  个不同的 Key 投影矩阵  来实现特定于分支的门控行为。对于具有隐藏状态  的第  个分支，特定于分支的门控信号计算如下：

然后，检索到的记忆由应用于共享值向量的这些独立门进行调节：。这种设计允许将线性投影（一个  和  个不同的 ）融合为单个密集的 FP8 矩阵乘法，最大限度地提高现代 GPU 的计算利用率。除非另有说明，所有实验都使用这种与流形约束超连接 (Manifold-Constrained Hyper-Connections, ) (Xie et al., 2025) 的集成。

图 3 | 稀疏分配和 Engram 缩放。左图：不同分配比率  下的验证损失。展示了两种计算预算（2e20 和 6e20 FLOPs）。两种机制都表现出 U 型，混合分配优于纯 MoE。右图：无限记忆机制下的缩放行为。验证损失相对于嵌入数量呈现对数线性趋势。

# 2.5. 系统效率：解耦计算与记忆

扩展增强记忆的模型通常受到 GPU 高带宽内存 (HBM) 有限容量的限制。然而，Engram 的确定性检索机制自然支持将参数存储与计算资源解耦。与 MoE 依赖运行时隐藏状态进行动态路由不同，Engram 的检索索引仅取决于输入 Token 序列。这种可预测性有助于针对训练和推理采取专门的优化策略，如图 2 所示。

在训练期间，为了适应大规模嵌入表，我们采用标准模型并行，将表分片到可用的 GPU 上。使用 All-to-All 通信原语在正向传递中收集活动行并在反向传递中分发梯度，使总内存容量随加速器数量线性扩展。

在推理期间，这种确定性使得**预取和重叠** (prefetch-and-overlap) 策略成为可能。由于记忆索引在正向传递之前即已知，系统可以通过 PCIe 异步地从充足的主机内存中检索嵌入。为了有效地掩盖通信延迟，Engram 模块被放置在骨干网络内的特定层，利用前几层的计算作为缓冲以防止 GPU 停顿。这需要一种硬件-算法联合设计策略：虽然将 Engram 放置得更深可以扩展可用于隐藏延迟的计算窗口，但我们在第 6.2 节的消融实验表明，建模性能倾向于早期干预以卸载局部模式重构。因此，最佳放置必须同时满足建模和系统延迟约束。

此外，自然语言 -grams 固有地遵循齐夫分布 (Zipfian distribution) (Chao and Zipf, 1950; Piantadosi, 2014)，其中一小部分模式占了绝大多数的记忆访问。这种统计特性激发了**多级缓存层次结构**：频繁访问的嵌入可以缓存在更快的存储层中（例如，GPU HBM 或主机 DRAM），而长尾的稀有模式则驻留在较慢的大容量介质中（例如，NVMe SSD）。这种分层允许 Engram 扩展到巨大的记忆容量，而对有效延迟的影响最小。

# 3. 缩放定律与稀疏分配

Engram 作为条件记忆的一种实例化，在结构上与 MoE 专家提供的条件计算互补。本节研究这种二元性的缩放属性以及如何最佳地分配稀疏容量。具体来说，两个关键问题驱动我们的研究：

1. **有限约束下的分配**：当总参数和训练计算量固定时（等参数和等 FLOPs），我们应该如何在 MoE 专家和 Engram 嵌入之间分配稀疏容量？
2. **无限记忆机制**：考虑到 Engram 非缩放的  开销，如果记忆预算被放宽或激进扩展，Engram 本身表现出什么缩放行为？

# 3.1. MoE 和 Engram 之间的最佳分配比率

**计算匹配公式** 我们使用三个参数指标分析权衡：

*  ：总可训练参数，不包括词汇表嵌入和 LM 头。
*  ：每个 Token 的激活参数。此数量决定了训练成本 (FLOPs)。
*  ：非活动参数，代表在不产生计算成本的情况下扩展模型大小的“免费”参数预算（例如，未选中的专家或未检索的嵌入）。

我们在每个 FLOPs 预算内保持  和  固定，以便模型具有相同数量的参数和相同的每 Token FLOPs。对于 MoE， 由前  个选定专家决定，而未选定专家的参数贡献给 。对于 Engram，每个 Token 仅检索固定数量的槽位，因此扩展嵌入槽位的数量会增加  而不增加每 Token FLOPs。

**分配比率** 我们定义分配比率  为分配给 MoE 专家容量的非活动参数预算的比例：

直观地说：

*  对应于纯 MoE 模型（所有非活动参数都是路由专家）。
*  减少路由专家的数量，并将释放的参数重新分配给 Engram 嵌入槽位。

**实验协议** 我们在两种计算机制下评估这种权衡，并在两种设置下保持恒定的稀疏比率 ：

*  FLOPs： 且 。基线 () 共有 106 个专家。
*  FLOPs： 且 。基线 () 共有 99 个专家。

对于不同的 ，我们仅通过调整路由专家的数量和 Engram 嵌入槽位的数量来构建相应的模型。所有运行都使用相同的训练流程和优化超参数。

**结果与分析** 图 3（左）揭示了验证损失与分配比率  之间一致的 U 型关系。值得注意的是，即使 MoE 分配减少到仅 （即 5.7B 模型总共 46 个专家，9.9B 模型总共 43 个专家），Engram 模型的性能仍与纯 MoE 基线（）相当。此外，纯 MoE 基线被证明是次优的：将大约  的稀疏参数预算重新分配给 Engram 会产生最佳性能。定量地，在 10B 机制 () 中，验证损失从 1.7248（在  时）提高到最优值附近的 1.7109（ 时）()。至关重要的是，该最优值的位置在不同机制下是稳定的（），表明在所检查的规模下（在固定稀疏度下）存在稳健的分配偏好。这种观察到的 U 型证实了两个模块之间的结构互补性：

* MoE 主导 ：模型缺乏用于静态模式的专用记忆，迫使它通过深度和计算低效地重构它们。
* Engram 主导 ：模型丧失了条件计算能力，损害了需要动态、上下文相关推理的任务；在这种机制下，记忆无法替代计算。

# 3.2. 无限记忆机制下的 Engram

在 3.1 节中，我们在固定参数预算下优化了分配。我们现在探索互补的设置：激进的记忆扩展。这项调查的动机是第 2.5 节详述的 Engram 独特的存储与计算解耦能力。

**实验协议** 我们利用一个固定的 MoE 骨干网络，其  且 ，训练 1000 亿 Token 以确保收敛。在该骨干网络之上，我们附加一个 Engram 表，并将槽位数  从  扫描到 （增加多达  亿参数）。作为基线，我们与 OverEncoding (Huang et al., 2025a) 进行比较，后者通过与词汇表嵌入平均来集成 -gram 嵌入。我们注意到，虽然其他工作如 SCONE (Yu et al., 2025) 也研究了大规模嵌入，但它主要侧重于推理，并且包含额外的模块（f-gram 模型）和额外的训练 FLOPs，使其与本研究的严格等计算约束不兼容。

**结果** 图 3（右）表明，扩展记忆槽位的数量会带来验证损失的清晰且一致的改善。在探索的范围内，曲线遵循严格的幂律（对数空间中的线性），表明 Engram 提供了一个可预测的缩放旋钮：更大的记忆量持续带来回报，而无需额外的计算。至关重要的是，关于缩放效率：虽然 OverEncoding 的直接平均方法受益于更大的记忆表，但 Engram 从相同的记忆预算中释放了更大的缩放潜力。连同第 3.1 节中的分配定律，这些结果验证了条件记忆是一种独特的、可扩展的稀疏容量轴，它补充了 MoE 的条件计算。

# 4. 大规模预训练

凭借提出的 Engram 架构和经验推导的分配定律，我们将 Engram 扩展到数十亿参数级别，以验证其在现实世界语言模型预训练中的功效。具体来说，我们训练了四个模型：(1) Dense-4B (41 亿总参数)，(2) MoE-27B (267 亿总参数)，(3) Engram-27B (267 亿总参数)，以及 (4) Engram-40B (395 亿总参数)。所有模型都使用相同的数据课程（相同的 Token 预算和顺序）进行训练，并且在激活参数的数量上严格匹配。

Table 1 | 密集模型、MoE 和 Engram 模型之间的预训练性能比较。所有模型均训练了 2620 亿 Token，并且在激活参数 (3.8B) 上匹配。Engram-27B 通过将路由专家的参数  重新分配给 57 亿参数的 Engram 记忆，实现了与 MoE-27B 的等参数。Engram-40B 进一步增加了 Engram 记忆 (185 亿参数)，同时保持激活参数预算不变。完整的训练时基准轨迹在附录 B 中报告。

<table><tr><td></td><td>基准 (指标)</td><td># Shots</td><td>Dense-4B</td><td>MoE-27B</td><td>Engram-27B</td><td>Engram-40B</td></tr><tr><td></td><td># 总参数</td><td></td><td>4.1B</td><td>26.7B</td><td>26.7B</td><td>39.5B</td></tr><tr><td></td><td># 激活参数 (无 token 嵌入)</td><td></td><td>3.8B</td><td>3.8B</td><td>3.8B</td><td>3.8B</td></tr><tr><td></td><td># 训练 Tokens</td><td></td><td>262B</td><td>262B</td><td>262B</td><td>262B</td></tr><tr><td></td><td># 专家 (共享 + 路由, top-k)</td><td></td><td>-</td><td>2 + 72 (top-6)</td><td>2 + 55 (top-6)</td><td>2 + 55 (top-6)</td></tr><tr><td></td><td># Engram 参数</td><td></td><td>-</td><td>-</td><td>5.7B</td><td>18.5B</td></tr><tr><td>语言</td><td>Pile (loss)</td><td>-</td><td>2.091</td><td>1.960</td><td>1.950</td><td>1.942</td></tr><tr><td>建模</td><td>验证集 (loss)</td><td>-</td><td>1.768</td><td>1.634</td><td>1.622</td><td>1.610</td></tr><tr><td rowspan="16">知识与推理</td><td>MMLU (Acc.)</td><td>5-shot</td><td>48.6</td><td>57.4</td><td>60.4</td><td>60.6</td></tr><tr><td>MMLU-Redux (Acc.)</td><td>5-shot</td><td>50.7</td><td>60.6</td><td>64.0</td><td>64.5</td></tr><tr><td>MMLU-Pro (Acc.)</td><td>5-shot</td><td>21.1</td><td>28.3</td><td>30.1</td><td>31.3</td></tr><tr><td>CMMLU (Acc.)</td><td>5-shot</td><td>47.9</td><td>57.9</td><td>61.9</td><td>63.4</td></tr><tr><td>C-Eval (Acc.)</td><td>5-shot</td><td>46.9</td><td>58.0</td><td>62.7</td><td>63.3</td></tr><tr><td>AGIEval (Acc.)</td><td>0-shot</td><td>29.1</td><td>38.6</td><td>41.8</td><td>45.9</td></tr><tr><td>ARC-Easy (Acc.)</td><td>25-shot</td><td>76.8</td><td>86.5</td><td>89.0</td><td>90.1</td></tr><tr><td>ARC-Challenge (Acc.)</td><td>25-shot</td><td>59.3</td><td>70.1</td><td>73.8</td><td>76.4</td></tr><tr><td>TriviaQA (EM)</td><td>5-shot</td><td>33.0</td><td>48.8</td><td>50.7</td><td>51.8</td></tr><tr><td>TriviaQA-ZH (EM)</td><td>5-shot</td><td>62.8</td><td>74.8</td><td>76.3</td><td>77.9</td></tr><tr><td>PopQA (EM)</td><td>15-shot</td><td>15.1</td><td>19.2</td><td>19.4</td><td>21.2</td></tr><tr><td>CCPM (Acc.)</td><td>0-shot</td><td>72.2</td><td>79.6</td><td>87.1</td><td>87.7</td></tr><tr><td>BBH (EM)</td><td>3-shot</td><td>42.8</td><td>50.9</td><td>55.9</td><td>57.5</td></tr><tr><td>HellaSwag (Acc.)</td><td>0-shot</td><td>64.3</td><td>71.8</td><td>72.7</td><td>73.1</td></tr><tr><td>PIQA (Acc.)</td><td>0-shot</td><td>63.8</td><td>71.9</td><td>73.5</td><td>76.5</td></tr><tr><td>WinoGrande (Acc.)</td><td>5-shot</td><td>64.0</td><td>67.6</td><td>67.8</td><td>68.1</td></tr><tr><td rowspan="4">阅读理解</td><td>DROP (F1)</td><td>1-shot</td><td>41.6</td><td>55.7</td><td>59.0</td><td>60.7</td></tr><tr><td>RACE-Middle (Acc.)</td><td>5-shot</td><td>72.4</td><td>80.9</td><td>82.8</td><td>83.3</td></tr><tr><td>RACE-High (Acc.)</td><td>5-shot</td><td>66.0</td><td>75.4</td><td>78.2</td><td>79.2</td></tr><tr><td>C3 (Acc.)</td><td>0-shot</td><td>57.7</td><td>60.1</td><td>63.6</td><td>61.8</td></tr><tr><td rowspan="7">代码与数学</td><td>HumanEval (Pass@1)</td><td>0-shot</td><td>26.8</td><td>37.8</td><td>40.8</td><td>38.4</td></tr><tr><td>MBPP (Pass@1)</td><td>3-shot</td><td>35.4</td><td>46.6</td><td>48.2</td><td>46.2</td></tr><tr><td>CruxEval-i (EM)</td><td>0-shot</td><td>27.6</td><td>30.7</td><td>32.2</td><td>36.2</td></tr><tr><td>CruxEval-o (EM)</td><td>0-shot</td><td>28.7</td><td>34.1</td><td>35.0</td><td>35.3</td></tr><tr><td>GSM8K (EM)</td><td>8-shot</td><td>35.5</td><td>58.4</td><td>60.6</td><td>62.6</td></tr><tr><td>MGSM (EM)</td><td>8-shot</td><td>27.0</td><td>46.8</td><td>49.4</td><td>52.4</td></tr><tr><td>MATH (EM)</td><td>4-shot</td><td>15.2</td><td>28.3</td><td>30.7</td><td>30.6</td></tr></table>

# 4.1. 实验设置

**训练数据和模型配置** 所有模型均在 2620 亿 Token 的语料库上进行预训练，我们使用 DeepSeek-v3 (Liu et al., 2024a) 的 Tokenizer，词汇量为 128k。对于建模，为了确保受控比较，除非另有明确说明，否则我们在所有模型中坚持一致的默认设置。我们使用一个 30 块的 Transformer，隐藏层大小为 2560。每个块集成了一个具有 32 个头的多头潜在注意力 (MLA) (DeepSeek-AI et al., 2024)，通过扩展率为 4 的 mHC (Xie et al., 2025) 连接到 FFN。所有模型均使用 Muon (Jordan et al., 2024; Team et al., 2025) 进行优化；详细的超参数列于附录 A。我们实例化了四个不同的模型：

* **Dense-4B** 作为基线模型。它利用上述骨干架构，在每个块中包含一个标准密集 FFN。
* **MoE-27B** 将标准密集 FFN 替换为 DeepSeekMoE 模块 (Dai et al., 2024)。配置有 72 个路由专家和 2 个共享专家（每个 Token 激活前  个路由专家），该模型扩展到 267 亿总参数，同时保持与 Dense-4B 相同的激活参数。
* **Engram-27B** 严格源自 MoE-27B 架构以确保公平比较。我们将路由专家的数量从 72 减少到 55，并将释放的参数重新分配给 57 亿参数的嵌入模块 ()，保持总模型大小恒定在 267 亿。关于 Engram 配置，我们在第 2 层和第 15 层实例化该模块，并将最大 -gram 大小设置为 3，头数设置为 8，维度设置为 1280。对于优化，嵌入参数使用 Adam (Kingma, 2014) 更新，学习率放大  且无权重衰减，而卷积参数初始化为零。
  Engram-40B 保留了与 Engram-27B 相同的骨干网络和计算预算，但将稀疏嵌入模块扩展到 185 亿参数（总计 395 亿参数）。该模型旨在研究 Engram 的缩放属性。

**评估协议** 我们在涵盖语言建模、知识、推理、阅读理解和代码/数学的各种基准测试套件上评估模型。对于每个基准测试，我们遵循标准的提示协议和评估指标。

**语言建模**：我们报告 The Pile (Gao et al., 2020) 测试集以及从与训练数据相同分布中提取的验证集上的损失。
**知识与推理**：MMLU (Hendrycks et al., 2021a), MMLU-Redux (Gema et al., 2025), MMLU-Pro (Wang et al., 2024b), CMMLU (Li et al., 2024), C-Eval (Huang et al., 2023), AGIEval (Zhong et al., 2024), ARC-Easy/Challenge (Clark et al., 2018), TriviaQA (Joshi et al., 2017), TriviaQA-ZH (内部), PopQA (Mallen et al., 2023), CCPM (Li et al., 2021), BBH (Suzgun et al., 2023), HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020), 和 WinoGrande (Sakaguchi et al., 2021)。
**阅读理解**：DROP (Dua et al., 2019), RACE (Middle/High) (Lai et al., 2017), 和 C3 (Sun et al., 2020)。
**代码与数学**：HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), CruxEval (Gu et al., 2024), GSM8K (Cobbe et al., 2021), MGSM (Shi et al., 2023), 和 MATH (Hendrycks et al., 2021b)。

# 4.2. 实验结果

Table 1 总结了主要结果。首先，与现有文献 (Borgeaud et al., 2022; He, 2024; Shazeer et al., 2017) 一致，稀疏架构表现出比密集模型更优越的缩放定律。在相同的训练计算预算下，所有三种稀疏变体 (MoE-27B, Engram-27B/40B) 在所有基准测试中都显著优于等 FLOPs 的 Dense-4B 基线。

Table 2 | 长上下文性能比较。括号中的值（例如 ）表示长上下文扩展之前的预训练步数和相应的损失。两个主要发现：（1）仅使用  的预训练 FLOPs（41k 对 50k），Engram-27B 就达到了与基线相当的 LongPPL (Fang et al.) 性能，同时在 RULER (Hsieh et al.) 上实现了显著更高的准确率；（2）在等预训练损失 (46k) 和等预训练 FLOPs (50k) 设置下，Engram-27B 在所有指标上都大幅优于基线。粗体表示最佳，下划线表示第二佳。

<table><tr><td>Model</td><td>LongPPL (32k)</td><td colspan="4">RULER (32k)</td></tr><tr><td></td><td>Perplexity (↓)</td><td>NIAH Accuracy (↑)</td><td>Other Tasks (↑)</td></tr><tr><td></td><td>Book</td><td>Paper</td><td>Code</td><td>L-CoT</td><td>S</td><td>MK</td><td>MV</td><td>MQ</td><td>VT</td><td>CWE</td><td>FWE</td></tr><tr><td>MoE-27B (50k, 1.63)</td><td>4.38</td><td>2.91</td><td>2.49</td><td>14.16</td><td>100.0</td><td>88.0</td><td>92.7</td><td>84.2</td><td>77.0</td><td>4.5</td><td>73.0</td></tr><tr><td>Engram-27B (41k, 1.66)</td><td>4.37</td><td>2.92</td><td>2.50</td><td>14.26</td><td>99.6</td><td>88.3</td><td>93.0</td><td>89.5</td><td>83.2</td><td>3.8</td><td>99.6</td></tr><tr><td>Engram-27B (46k, 1.63)</td><td>4.19</td><td>2.84</td><td>2.45</td><td>13.59</td><td>97.6</td><td>89.0</td><td>95.5</td><td>97.0</td><td>87.2</td><td>4.3</td><td>98.6</td></tr><tr><td>Engram-27B (50k, 1.62)</td><td>4.14</td><td>2.82</td><td>2.44</td><td>13.41</td><td>99.3</td><td>89.3</td><td>96.5</td><td>97.0</td><td>89.0</td><td>5.9</td><td>99.3</td></tr></table>

更重要的是，Engram-27B 持续优于等参数和等 FLOPs 的 MoE-27B 基线。有趣的是，这些收益并不局限于知识密集型任务（例如，MMLU: +3.0, MMLU-Pro: +1.8, CMMLU: +4.0），在这些任务中，记忆容量直观上是有益的。我们观察到在通用推理领域（例如，BBH: +5.0, ARC-Challenge: +3.7, DROP: +3.3）以及代码和数学推理（例如，HumanEval: +3.0, MBPP: +1.6, GSM8K: +2.2, MATH: +2.4）有更显著的提升。为了减少基准测试噪声的影响并可视化训练动态，我们在附录 B 中提供了预训练期间完整的基准测试轨迹。这些结果支持了我们的假设，即引入专用的知识查找原语提高了表示效率，超出了将整个稀疏预算分配给条件计算所能达到的效果。

最后，扩展到 Engram-40B 进一步降低了预训练损失并提高了大多数基准测试的性能。虽然它在每个任务上尚未严格主导 Engram-27B，但这很可能是训练不足的人为产物。我们观察到 Engram-40B 与基线之间的训练损失差距在训练结束时继续扩大，这表明扩展的记忆容量在当前的 Token 预算内尚未完全饱和。

# 5. 长上下文训练

通过将局部依赖建模卸载到静态查找，Engram 架构保留了宝贵的注意力容量来管理全局上下文。在本节中，我们通过进行长上下文扩展训练 (Gao et al., 2025; Peng et al., 2024) 来实证验证这一结构优势。通过严格的评估协议，将架构贡献与基础模型能力隔离开来，我们证明了 Engram 在长程检索和推理任务中产生了显著收益。

# 5.1. 实验设置

**训练细节** 为了实现长上下文能力，我们采用了 DeepSeek-V3 (Liu et al., 2024a) 中引入的上下文扩展策略。在预训练阶段之后，我们应用 YaRN (Peng et al., 2024) 进行上下文窗口扩展，在 32768 Token 上下文训练阶段进行 5,000 步（300 亿高质量长上下文 Token）。超参数为 scale  且缩放因子 。

**模型配置** 我们比较了四种不同模型配置的上下文扩展。我们使用 MoE-27B 和 Engram-27B 的最终预训练检查点（50k 步）。此外，为了严格基准测试架构效率，我们选择了 Engram-27B 在 41k 和 46k 步的两个中间检查点。尽管初始化阶段不同，但所有变体都经历了完全相同的上下文扩展训练协议。至关重要的是，选择 Engram-27B (46k) 是因为它表现出与完全训练的 MoE-27B (50k) 相同的预训练损失。这创造了一个受控的“等损失” (Iso-Loss) 设置，确保上下文扩展期间的任何性能差异都归因于架构，而不是模型的起始质量。

**评估基准** 我们使用 LongPPL (Fang et al.) 和 RULER (Hsieh et al.) 评估长上下文性能。对于 LongPPL，我们构建了涵盖四类别的评估集：长书、研究论文、代码仓库和长思维链 (CoT) 轨迹。对于 RULER，我们评估了聚合成 8 个类别的 14 个子集：单 (S)、多键 (MK)、多值 (MV) 和多查询 (MQ) 大海捞针 (Needle-in-a-Haystack)；多跳变量跟踪 (VT)、常用词提取 (CWE)、频繁词提取 (FWE) 和问答 (QA)。

# 5.2. 实验结果

评估结果总结在表 2 中。为了准确评估 Engram 架构的贡献，我们的分析分两步进行：首先，将基础模型能力的影响与架构设计解耦，其次，进行受控分析。

**超越注意力机制的长上下文能力** 虽然注意力机制和位置编码提供了上下文处理的结构基础 (Press et al., 2022; Su et al., 2024; Xiao et al., 2024; Yang et al., 2025)，但我们的结果表明，长上下文性能并非完全由架构先验决定。观察 Engram  的轨迹，我们发现长上下文性能随着预训练进程单调提高，即使在控制了相同的模型架构和上下文扩展阶段固定的计算预算时也是如此。这表明长上下文性能与基础模型的通用建模能力内在耦合。因此，严格的架构比较必须通过对齐基础模型损失来控制这一混杂变量，而不仅仅是对齐训练步数。

**受控设置下的架构优势** 在上述原则的指导下，我们将 Engram 与 MoE 基线进行基准测试。当控制基础能力时，Engram 模块的效率收益变得明显：

* **等损失设置 (46k vs. 基线)**：此设置严格隔离了架构效率。当比较 Engram-27B (46k) 与完全训练的 MoE-27B (50k)——在预训练损失上对齐的模型——时，Engram 表现出显著收益。具体来说，它在复杂检索任务上优于基线（例如，Multi-Query NIAH: 97.0 vs. 84.2; VT: 87.2 vs. 77.0）。
* **等 FLOPs 设置 (50k vs. 基线)**：在标准等计算预算下，Engram-27B (50k) 进一步扩大了这一差距，确立了全面的最高性能。
* **极端设置 ( 计算)**：即使是提前停止的 Engram-27B (41k) 与完全训练的 MoE-27B (50k) 相比也极具竞争力。它在 LongPPL 上与基线相当，在 RULER 上超过基线，强调了 Engram 架构的内在优势。

(a) LogitLens 逐层 KL 散度

(b) CKA 图: Engram-27B vs MoE-27B

(c) CKA 图: Engram-40B vs MoE-27B

图 4 | 表示对齐和收敛速度的分析。(a) 通过 LogitLens (nostalgebraist, 2020) 计算的逐层 KL 散度。早期层中持续较低的散度表明 Engram 加速了预测收敛。(b-c) 由 CKA (Kornblith et al., 2019) 计算的相似度热图。高相似度对角线的明显向上偏移表明 Engram 的浅层在功能上等同于 MoE 模型的更深层，有效地增加了模型的深度。

# 6. 分析

在本节中，我们将研究 Engram 的内部机制，包括其有效深度（6.1 节）、核心模块设计（6.2 节）和参数敏感性（6.3 节）。此外，我们评估了卸载时的推理吞吐量（6.4 节），并以案例研究结束（6.5 节）。

# 6.1. Engram 在功能上是否等同于增加模型深度？

当前的 LLM 缺乏专用的知识查找原语，它们依赖计算来模拟记忆回忆。如表 3 所示，为了识别实体“Diana, Princess of Wales”，LLM 必须消耗多层注意力和 FFN 来逐步组合特征 (Ghandeharioun et al., 2024; Jin et al., 2025; Li and Subramani, 2025)，这一过程理论上可以通过知识查找操作来识别。

鉴于此，我们假设通过为模型配备显式的知识查找能力，Engram 通过减轻模型早期阶段的特征组合负担，有效地模拟了模型深度的增加。为了验证这一假设，我们使用了两种机制可解释性工具：LogitLens (Belrose et al., 2023; nostalgicbraist, 2020) 和中心核对齐分析 (CKA) (Davari et al., 2023; Kornblith et al., 2019)。

# 6.1.1. 加速预测收敛

我们首先使用 LogitLens (nostalgebraist, 2020) 分析跨层预测的演变。通过用最终的 LM 头投影每个中间层的隐藏状态，我们计算中间输出分布与模型最终输出分布之间的 Kullback-Leibler 散度 (Kullback and Leibler, 1951)。该指标量化了潜在表示接近“准备好预测”的程度 (Belrose et al., 2023; Csordás et al., 2025)。

图 4 (a) 报告了逐层 KL 散度。与 MoE 基线相比，两种 Engram 变体都表现出系统性更小的 KL 散度，最明显的差距出现在早期块中。

Table 3 | 重现自 Ghandeharioun et al. (2024) 的实体解析示例。此表说明了 LLM 如何通过注意力层和 FFN 逐渐整合上下文 Token，以构建实体 "Diana, Princess of Wales" 的内部表示。“潜在状态翻译”列显示了 PatchScope (Ghandeharioun et al., 2024) 为最后一个 Token 自动生成的文本：“Wales”，而“解释”列呈现了原作者提供的手动解释。

<table><tr><td>Layer</td><td>Latent State Translation</td><td>Explanation</td></tr><tr><td>1-2:</td><td>Country in the United Kingdom</td><td>Wales</td></tr><tr><td>3:</td><td>Country in Europe</td><td>Wales</td></tr><tr><td>4:</td><td>Title held by female sovereigns in their own right or by queens consort</td><td>Princess of Wales (unspecific)</td></tr><tr><td>5:</td><td>Title given to the wife of the Prince of Wales (and later King)</td><td>Princess of Wales (unspecific)</td></tr><tr><td>6:</td><td>Diana, Princess of Wales (1961-1997), the first wife of Prince Charles, Prince of Wales, who was famous for her beauty and humanitarian work</td><td>Diana, Princess of Wales</td></tr></table>

Engram 曲线中更陡峭的下降表明模型更快地完成了特征组合。这一观察结果与我们的假设一致：通过显式访问外部知识，Engram 减少了所需的计算步骤，从而在网络层次结构中更早地达到高置信度、有效的预测。

# 6.1.2. 表示对齐和有效深度

为了进一步研究 Engram 层在语义上是否对应于基线的更深层，我们采用了中心核对齐 (CKA)，这是一种广泛建立的用于比较表示结构的指标 (Kornblith et al., 2019; Kriegeskorte et al., 2008)。给定两组表示  和 （例如，来自不同模型或层的激活），CKA 定义为：

其中  和  表示 Gram 矩阵（使用线性核），HSIC 是希尔伯特-施密特独立性准则 (Gretton et al., 2005)。我们采用了具有 HSIC 无偏估计量的小批量实现 (Davari et al., 2023)，并在 Few-NERD 数据集 (Ding et al., 2021) 上进行评估，提取对应于命名实体最终 Token 的隐藏状态。

为了严格量化逐层对应关系，我们首先计算成对 CKA 相似度矩阵 ，其中  是层数。然后我们引入软对齐指数 ，定义为每个 Engram 层  的前  个最相似 MoE 层的加权质心：

这里， 表示 MoE 层  和 Engram 层  之间的相似度得分。该指数 

图 5 | 架构消融结果。我们在两种设置下比较了 3B MoE 基线与 Engram 变体：(1) 层敏感性（深蓝色曲线）：扫描单个 Engram 模块的插入深度确认早期注入（第 2 层）是最佳的，而在更深层效果会降低。(2) 组件消融（右侧标记）：从参考配置中移除子模块证明了多分支集成、Tokenizer 压缩和上下文感知门控的重要性。

作为对应于 Engram 层  的“有效 MoE 深度”的稳健代理，利用 top- 过滤（）来减轻低相似度噪声。

图 4 (b)-(c) 可视化了叠加了软对齐曲线（虚线白线）的相似度热图。我们观察到明显的对角线向上偏移，这意味着对于大范围的层，。例如，Engram-27B 第 5 层形成的表示与 MoE 基线大约第 12 层的表示最紧密地对齐。

这种一致的非对角线偏移与 LogitLens 结果（6.1.1 节）一致，证实了 Engram 在更早的层实现了更深层的表示。这验证了我们的中心假设：通过显式查找绕过早期特征组合，Engram 在功能上等同于增加模型的有效深度。

# 6.2. 结构消融和层敏感性

在本节中，我们在受控设置下对 Engram 进行消融，以研究每个关键模块设计的有效性。除非另有说明，骨干网络是训练了 1000 亿 Token 的 12 层 3B MoE 模型（0.56B 激活参数）。图 5 报告了验证损失。橙色虚线表示 3B MoE 基线 (Val Loss = 1.808)。

**参考配置** 我们用固定的 16 亿参数 Engram 记忆增强骨干网络。我们的参考模型使用 -grams 并在第 2 层和第 6 层插入 Engram，实现了 Val Loss ，相比 MoE 基线有显著改进 ()。以下所有结构消融均相对于此参考定义。

**记忆应该注入哪里？** 为了研究深度敏感性，我们保持 Engram 预算固定 (1.6B) 但将其合并到一个 Engram 模块中，并将其插入层从 1 扫描到 12（图 5 中的深蓝色“Layer Sweep”曲线）。该实验揭示了 Engram 放置的固有权衡。

**放置权衡** 早期注入 Engram 允许其在骨干网络消耗计算深度之前卸载局部模式重构，这与骨干网络的自然分层处理相一致 (Ghandeharioun et al., 2024; Jin et al., 2025; Li and Subramani, 2025; Tenney et al., 2019)。然而，这会导致门控精度的代价：早期隐藏状态尚未通过注意力聚合足够的全局上下文，并且并行分支缺乏细粒度调节所需的表示差异 (Xie et al., 2025; Zhu et al., 2025)。因此，最佳放置需要平衡 (i) 尽早卸载静态局部模式和 (ii) 利用更强的上下文查询进行后续门控。

扫描显示第 2 层实现了最佳的单层性能 (Val Loss = 1.770)，优于第 1 层，并随着插入点变深而降低。这表明一轮注意力已经足以提供有意义的上下文  进行门控，同时仍然足够早以替代骨干网络的底层局部聚合。

虽然第 2 层在单次注入约束下是最佳的，但我们发现将相同的 1.6B 记忆分成两个较小的模块（通过减少嵌入维度  实现）并放置在第 2 层和第 6 层表现更好 (Val Loss = 1.768)。这种分层设计通过结合早期干预和丰富的后期上下文门控协调了权衡。更重要的是，分层插入还提供了实际的系统优势，能够更好地利用第 2.5 节中讨论的内存层次结构。

**哪些组件重要？** 从参考配置开始，我们在保持 Engram 参数预算固定的同时消融单个设计选择。结果由图 5 中的标记表示。我们发现三个组件产生最显著的收益：(i) 多分支骨干网络内的特定分支融合，(ii) 上下文感知门控，以及 (iii) Tokenizer 压缩。移除其中任何一个都会导致验证损失的最大退步。具体来说，对于“w/o multi branch”消融，我们保留 mHC 骨干结构，但将特定分支的门控替换为应用于预映射  后隐藏状态的单个 Engram 融合 (Xie et al., 2025)。

其他变化的影响较小：移除轻量级深度卷积只会轻微降低性能。在固定的 1.6B 预算下分配容量给 4-grams 略显次优——可能是因为它稀释了更频繁的 2/3-gram 模式的容量——尽管我们不排除更高阶 -grams 在更大记忆规模下变得有益。

# 6.3. 敏感性分析

为了表征 Engram 模块的功能贡献，我们在推理过程中完全抑制稀疏嵌入输出，同时保持骨干网络不变，以此来评估模型。至关重要的是，这种事后消融导致了训练-推理的不一致，可能会在复杂的混合能力任务中引入噪声。因此，我们优先分析**事实性知识**和**阅读理解**——敏感性谱系的两个极端——在这个压力测试下表现出最高的信噪比。

如图 6 所示，结果揭示了鲜明的功能二分法。事实性知识

图 6 | Engram 消融下的保留性能。事实性知识严重依赖 Engram 模块，而阅读理解主要由骨干网络保留。

基准遭受灾难性崩溃，仅保留了原始性能的 （例如，TriviaQA 为 ），证实了 Engram 模块充当参数知识的主要存储库。相反，阅读理解任务非常有弹性，保留了 （例如，C3 为 ），这表明基于上下文的任务主要依赖于骨干网络的注意力机制而不是 Engram。

# 6.4. 系统效率

Engram 优于基于路由的 MoE 的一个关键系统优势是其稀疏激活由显式、静态的哈希 ID 寻址。这产生了严格确定的内存访问模式：一旦 Token 序列已知，下一个 Engram 查找的索引就固定了，并且可以在相应层执行之前计算。

**实验设置** 我们基于 nano-vLLM（行业标准 vLLM 引擎 (Kwon et al., 2023) 的简化原型）实现了推理工具。为了获得干净的延迟基线，排除 MoE 中专家并行带来的混杂通信模式，我们在两个密集骨干网络（Dense-4B 和 Dense-8B）上进行基准测试。我们将一个巨大的 1000 亿参数 Engram 层插入到第二个 Transformer 块中，整个嵌入表驻留在主机 DRAM 中。在推理过程中，系统异步预取 Engram 层的嵌入，将 PCIe 传输与第一个块的计算重叠。

**结果** 如表 4 所详述，卸载 1000 亿参数的嵌入表产生的吞吐量惩罚可以忽略不计，在 8B 骨干网络上最高仅为 。这证实了早期密集块的计算强度提供了足够的时间窗口来掩盖检索延迟。至关重要的是，每步的有效通信量随激活槽位数扩展，而不是随总嵌入表大小扩展。

至关重要的是，该实验作为一个保守的基线。虽然第 2.5 节中的分层设计利用齐夫局部性将频繁项缓存在 HBM 中，但我们的实验设置强制所有检索通过 PCIe 总线从主机内存传输。这种基线

Table 4 | 端到端推理吞吐量。我们在整个 1000 亿参数 Engram 层卸载到主机内存的情况下测量推理吞吐量。

<table><tr><td colspan="2">实验设置</td><td colspan="2">吞吐量结果</td></tr><tr><td>Hardware</td><td>NVIDIA H800</td><td>Base Model</td><td>4B-Dense</td><td>8B-Dense</td></tr><tr><td>Workload</td><td>512 Sequences</td><td>Configuration</td><td>Throughput (tok/s)</td><td>Throughput (tok/s)</td></tr><tr><td>Sequence Length</td><td>Uniform(100, 1024)</td><td>Baseline</td><td>9,031.62</td><td>6,315.52</td></tr><tr><td></td><td></td><td>+ 100B Engram (CPU Offload)</td><td>8,858.28</td><td>6,140.02</td></tr></table>

检索策略产生最小开销的事实强烈表明，完全优化、感知局部性的实现将产生可忽略不计的吞吐量惩罚。

# 6.5. 案例研究：门控可视化

在 2.3 节中，我们介绍了上下文感知门控机制，旨在动态调节检索到的静态记忆到骨干网络的集成。为了实证验证 Engram 是否按预期行为，我们在图 7 中可视化了各种样本中 Engram-27B 的门控标量 。

结果显示了明显的选择性模式。门控机制在完成局部、静态模式时持续激活（以红色显示）。在英语中，我们在多 Token 命名实体（例如 "Alexander the Great", "the Milky Way"）和公式化短语（例如 "By the way", "Princess of Wales"）上观察到强烈的激活。这种行为有效地推广到各种语言。在中文示例中，Engram 识别并检索独特的成语和历史实体，例如“Four Great Inventions” (四大发明) 和“Zhang Zhongjing” (张仲景)。这些定性结果证实了 Engram 成功识别并处理刻板的语言依赖关系，有效地减轻了 Transformer 骨干网络记忆这些静态关联的负担。

7. 相关工作

N-gram 建模与嵌入扩展。源于香农框架（Shannon, 1948），N-gram 模型依赖局部历史来预测标记，传统上采用平滑技术（Katz, 1987; Kneser and Ney, 1995）以缓解数据稀疏性问题。尽管范式已转向神经架构（Bengio 等, 2003）以捕捉长距离依赖关系，但 N-gram 查找的计算效率在现代表示学习中得以保留，例如开创性工作 FastText（Bojanowski 等, 2017）。

图 7 | Engram 的门控机制可视化。热力图强度对应门控标量 alpha_{t} in [0,1] 的大小，颜色越深的红色表示激活越强。由于 Engram 在后缀 N-gram（此处 N = 3）上操作，特定标记 x_{t} 上的高激活意味着其前面的标记（如以 t 结尾的短语）被识别为可从内存中有效检索的静态模式。

最近，这一范式以嵌入扩展的形式重新兴起。虽然 PerLayer Embeddings（Team, 2025）和 DeepEmbed（RWKV Team, 2025）等架构通过大规模表扩展容量，但另一条开创性研究路线——与我们的方法最相关——将组合式 N-gram 结构直接整合到表示空间中。SuperBPE（Liu 等, 2025）和 SCONE（Yu 等, 2025）明确针对高频模式：前者通过将多词表达式合并为“超词”标记，后者则通过辅助编码模型实现。同时，OverEncoding（Huang 等, 2025a）和字节潜在变换器（BLT）（Pagnoni 等, 2025）分别采用哈希 N-gram 嵌入，在标记和字节级别捕捉局部依赖关系。这些研究共同证明了通过 N-gram 表示扩展参数的有效性，且计算开销极小。尽管这些方法在其各自设置中带来了显著收益，但我们的工作在两个关键维度上存在根本性差异。

- 首先，关于建模和评估协议。先前的方法通常将 N-gram 嵌入视为外部增强，而未在严格公平的比较协议下验证其效率。例如，SCONE（Yu 等, 2025）专注于推理，并依赖于增加训练 FLOPs 的辅助模块。类似地，OverEncoding（Huang 等, 2025a）即使在非等参数设置下，也无法在稀疏 MoE 主干上产生有意义的改进。相比之下，我们将条件内存视为一种首要建模范式，通过精心设计的 Engram 模块实例化。通过在我们的稀疏分配框架内严格评估该设计，我们证明了其相对于严格的等参数和等 FLOPs MoE 基线具有明显优势。

- 其次，从系统角度来看，我们倡导算法-系统协同设计。现有方法将嵌入严格置于输入层（第 0 层），这本质上将内存访问和计算序列化（Huang 等, 2025a; Yu 等, 2025）。相反，Engram 战略性地将内存注入更深层，以实现通信与计算的重叠。此外，通过利用 N-gram 固有的齐夫分布，我们可以最大化硬件内存层次结构的效用。这种整体设计使 Engram 能够扩展至海量参数，同时推理开销可忽略不计。

混合专家模型。MoE 架构通过按标记有条件地激活专家子集，解耦了模型容量与计算成本，这一范式由 Shazeer 等人（2017）提出。随后的创新，如 GShard（Lepikhin 等, 2020）、BASE（Lewis 等, 2021）、Switch Transformer（Fedus 等, 2022）和 GLaM（Du 等, 2022），实现了超线性参数扩展，同时保持恒定的推理成本。最近，DeepSeekMoE（Dai 等, 2024）展示了卓越的效率，通过细粒度专家分割和共享专家隔离，显著优于具有相同活跃参数的密集模型。采用此架构，最先进的模型如 DeepSeek-V3（Liu 等, 2024a）和 Kimi-k2（Team 等, 2025）已将总参数规模进一步推至数千亿级别。

内存网络。内存增强网络的研究旨在扩大模型容量而不成比例地增加计算成本，大致可分为参数化和非参数化方法。参数化内存方法，如 PKM（Lample 等, 2019）、PEER（He, 2024）、Selfmem（Cheng 等, 2023b）、Memory+（Berges 等, 2025）和 UltraMem（Huang 等, 2025b,c），将大规模稀疏键值存储直接集成到模型层中，从而在对 FLOPs 影响可忽略的情况下显著增加容量。相反，非参数化内存方法如 REALM（Guu 等, 2020）、RETRO（Borgeaud 等, 2022; Wang 等, 2023）和 PlugLM（Cheng 等, 2023a）将知识存储与模型处理解耦，将外部内存视为可编辑和可扩展的键值存储，使模型能够在无需重新训练的情况下适应不断变化的信息。

知识存储机制。在容量扩展的同时，大量研究深入探讨了 Transformer 如何编码和检索事实知识的内部机制。前馈网络（FFNs）被广泛假设为充当键值内存（Geva 等, 2021）。在此框架下，第一层充当模式检测器（“键”），而第二层将特定信息投影到残差流中（“值”）。这种模块化特性通过识别负责存储特定事实的特定“知识神经元”得到证实（Dai 等, 2022）。因果追踪方法进一步验证了这一点，这些方法将事实回忆的信息流映射到特定的 FFN 层（Meng 等, 2022）。这些见解催生了精确的模型编辑算法，如 ROME（Meng 等, 2022）和 MEMIT（Meng 等, 2023），允许在无需重新训练的情况下直接更新事实关联。此外，对内部表示的研究（如 Othello-GPT 中的研究，Li 等, 2023a）表明，这些存储机制可能促进结构化“世界模型”的出现，而非仅仅是统计记忆。

8. 结论

在本工作中，我们引入条件内存作为主流条件计算范式（MoE）的互补稀疏轴，旨在解决通过动态计算模拟知识检索的低效问题。我们通过 Engram 实现这一概念，该模块现代化了经典的 N-gram 嵌入，以实现对静态模式的可扩展、常数时间 O(1) 查找。

通过制定稀疏分配问题，我们揭示了一种 U 形扩展定律，表明在 MoE 专家和 Engram 内存之间进行混合稀疏容量分配严格优于纯 MoE 基线。受此定律指导，我们将 Engram 扩展至 27B 参数，在多个领域实现了卓越性能。值得注意的是，虽然内存模块直观上有助于知识检索，但我们观察到在通用推理、代码和数学方面获得了更大的收益。

我们的机制分析表明，Engram 通过减轻早期层的静态重建任务，有效地“加深”了网络，从而释放注意力容量以专注于全局上下文和复杂推理。这种架构转变转化为长上下文能力的显著提升，这体现在 LongPPL 和 RULER 性能的提升上。最后，Engram 提倡将基础设施感知效率作为首要设计原则。其确定性寻址允许存储与计算解耦，能够将海量参数表卸载到主机内存，同时推理开销可忽略不计。我们设想条件内存功能将成为下一代稀疏模型不可或缺的建模原语。
