# BitMoD：位串行混合数据类型 LLM 加速

Yuzong Chen†, Ahmed F. AbouElhamayed†, Xilai Dai†, Yang Wang‡, Marta Andronic§, George A. Constantinides§, and Mohamed S. Abdelfattah†

†康奈尔大学计算机系统实验室（Computer Systems Lab, Cornell University）

**$\ddagger$** 微软研究院系统与网络研究组（Systems and Networking Research Group, Microsoft Research）

**$^{\S}$** 帝国理工学院电气与电子工程系（Department of Electrical and Electronic Engineering, Imperial College London）

†{yc2367, afa55, xd44, mohamed} @cornell.edu

**$\ddagger$** yang.wang92@microsoft.com

**$\S$** {marta.andronic18, g.constantinides} @imperial.ac.uk

 **摘要** —大型语言模型（LLMs）在各种机器学习任务中展现出卓越的性能。然而，LLMs 庞大的内存占用严重阻碍了它们的部署。在本文中，我们通过  **BitMoD** `<sup>`1 `</sup>` 提升 LLMs 的可访问性，这是一种**算法-硬件协同设计**解决方案，可以在低权重精度下实现高效的 LLM 加速。在算法方面，BitMoD 引入了 **细粒度数据类型自适应** ，使用不同的数值数据类型来量化一组（例如 128 个）权重。通过对这些新数据类型的精心设计，BitMoD 能够在保持高精度的同时，将 LLM 权重量化到极低的精度（例如 4 位和 3 位）。在硬件方面，BitMoD 采用 **位串行处理单元** ，以轻松支持多种数值精度和数据类型；我们的硬件设计包括两项关键创新：首先，它采用**统一表示**来处理不同的权重数据类型，从而降低了硬件成本。其次，它采用**位串行反量化单元**以最小的硬件开销重新缩放每组部分和。我们对六个代表性 LLMs 的评估表明，BitMoD 显著优于现有的 LLM 量化和加速方法。对于判别式任务，BitMoD 可以将 LLM 权重量化到 4 位，平均精度损失 **$< 0.5\%$**。对于生成式任务，BitMoD 能够将 LLM 权重量化到 3 位，同时实现比现有 LLM 量化方案 **更好的困惑度** 。结合卓越的模型性能和高效的加速器设计，BitMoD 比现有的 LLM 加速器 ANT 和 OliVe 分别实现了平均 **$1.69\times$** 和 **$1.48\times$** 的 **加速比** 。

---

# I. 引言

大型语言模型（LLMs）在自然语言处理任务中取得了重大突破 [47], [57]。然而，LLM 规模和复杂性的增长速度继续超过现有硬件平台计算性能和内存容量的扩展速度 [22]。例如，2018 年推出的第一代 GPT 模型仅包含 1.17 亿个参数，而第二代和第三代在两年内分别增长了 **$10\times$** 和 **$1000\times$** [9]。这种规模的快速增长需要巨大的内存容量来进行模型部署，阻碍了它们的广泛采用，特别是在计算和内存资源有限的 **边缘场景** 。例如，最先进（SOTA）的开源 LLM 系列 Llama-3 [35] 包含

超过 80 亿个参数，需要超过 16GB 的内存以 16 位浮点（FP16）格式存储模型权重，这无法适应 Jetson-TX2 等只有 8GB 内存的边缘 GPU [37]。因此，设计新颖的 LLM 压缩算法，并结合为高效部署压缩模型而协同设计的加速器，是增强 LLMs 在边缘设备上可访问性的一个有前景的解决方案。

量化是缓解 LLMs 计算和内存需求的最硬件高效的方法之一。通常，有两种类型的量化机制。第一种是 **量化感知训练** （QAT），需要重新训练来更新模型权重和量化参数（例如，缩放因子）[26], [31]。第二种方法是 **训练后量化** （PTQ），它不需要重新训练 [10], [19], [20], [25], [30], [42], [52]。虽然 QAT 可以实现比 PTQ 更具竞争力的精度，但重新训练 LLMs 的高昂成本使其不切实际。因此，PTQ 通常被现有 LLM 量化研究所采用。尽管一些 PTQ 工作将权重和激活都量化到低精度 [25], [42], [52]，但**仅权重量化**可以在 LLMs 的边缘部署中提供更好的模型精度和硬件效率权衡，因为权重在内存占用中占主导地位 [10], [19], [20], [30]。然而，现有的仅权重量化工作在 GPU 上计算效率低下，**因为 GPU 缺乏专用的硬件来执行整数权重和浮点激活之间的乘法**。因此，这些方法必须首先将权重反量化为 FP16，并依靠浮点流水线进行计算。

为了实现 LLMs 更好的计算效率，最近的一项加速器工作 FIGNA [27] 提出了一系列专用计算单元，用于整数权重和浮点激活之间的混合精度算术。为了进一步释放量化在提高硬件效率方面的潜力，一些工作提出了基于**自定义低精度数据类型**的算法-硬件协同设计解决方案 [25], [26], [38], [40]。微缩放格式（MX）[38], [40] 将 8 位元数据作为共享指数分配给一组低精度权重。ANT [26] 引入了一种新的数据类型，能更好地适应张量内部的值分布，从而减少量化误差。OliVe [25] 提出了一种 **异常值-受害者对量化机制** ，其中具有大值的异常值以“自适应偏差浮点”（Adaptive Biased Float）格式表示，并通过修剪其相邻的、具有小值的受害者值来保护。

在本文中，我们提出了  **BitMoD$^2$** ，这是一种算法-硬件协同设计解决方案，用于在**低权重精度下实现高效的 LLM 加速**。在算法方面，BitMoD 利用**每组量化** [16]，并通过用一个**特殊值**重新利用冗余的零值来修改低精度浮点数据类型，这提供了使数据类型本身更好地适应每个权重组数值分布的能力。通过精心选择特殊值，BitMoD 能够在保持良好模型精度的同时，以微小的编码开销将 LLM 权重量化到极低的精度（例如 4 位和 3 位）。在硬件方面，BitMoD 采用 **位串行计算范式** ，并使用**统一表示**来处理不同的低精度数据类型，从而有效地权衡权重精度和硬件效率。

本文的主要贡献总结如下：

1. 我们提出了 BitMoD，一种硬件高效的 PTQ 解决方案，用于 LLM 加速。BitMoD 引入了新的数据类型，这些数据类型专为以微小的编码开销进行 4 位和 3 位精度的**每组权重量化**而定制。
2. 我们证明了所提出的数据类型可以与其他量化优化技术无缝集成，实现了比 SOTA 纯软件 LLM 量化工作 **更好的模型困惑度** 。
3. 我们提出了一种用于 BitMoD 的高效加速器设计，该设计对多种低精度数据类型采用了 **统一的位串行表示** 。这有效地减少了执行低精度权重和 FP16 激活之间计算的硬件成本，并以权重精度换取硬件效率的提高。
4. 我们对六个代表性 LLMs 的评估显示，与基准 FP16 加速器相比，BitMoD  **平均实现了 **$2.2\times$** 的加速比和 **$2.31\times$** 更好的能效** ，且没有精度损失。与 SOTA 加速器 ANT 和 OliVe 相比，BitMoD 分别实现了 **平均 **$1.69\times$** 和 **$1.48\times$** 的加速比** 。

---

# II. 背景与动机

# A. 为什么选择 LLMs 的权重量化？

为了证明 LLM 权重量化对于边缘应用的重要性，我们分析了四个代表性 LLMs 在批处理大小为 1 的情况下，运行判别式和生成式任务时，权重和激活的总内存访问足迹。对于判别式任务，LLM 接收输入上下文并输出单个 token，例如在情感分析 [46] 和多项选择问答 [13] 中。对于生成式任务，LLM 接收输入上下文并输出多个 token。我们分别将判别式和生成式任务的输入到输出序列长度设置为 **$256:1$** 和 **$256:256$**，以适应边缘应用，正如 Lin 等人 [30] 所建议的。如图 1 所示，LLM 权重的访问消耗的内存比激活的访问 **大几个数量级** 。尽管判别式任务只需要输出单个 token（例如，多项选择问答的“A”/“B”/“C”），但 LLM 的权重张量维度（例如，OPT-1.3B 为 2048）远大于输入 token 长度，导致内存访问由**权重**主导。此外，生成式任务需要为每个新的输出 token  **重复获取权重** ，导致 LLM 权重的内存访问显著更高。因此，权重量化对于在批处理大小小且输入 token 长度通常较短的边缘场景中部署 LLMs 更为有效。

**${}^{2}$** BitMoD 代表位串行计算与混合数据类型 (Bit-serial computation with Mixture of Data types)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-01/f8d4377d-a546-44e7-bc70-e1a41e4a8164/b15b1ae88cbc681e0ef14f9fb53c75db24d9832e955f6e6c8d05f6570286dd0f.jpg)

Fig. 1: 判别式任务（256 个输入 token 和 1 个输出 token）和生成式任务（256 个输入 token 和 256 个生成的 token）中，权重和激活的总内存访问。请注意 y 轴上的对数刻度。请注意，尽管 KV-缓存比判别式任务大得多，但在批处理大小为 1 的生成式任务中，权重和激活内存访问之间的差距会增加。虽然先前的工作 [44] 正确地报告了由 KV-缓存引起的内存瓶颈，但这仅发生在 **$175\mathrm{B}+$** 参数模型且具有高批处理大小（例如 512）和超过 512 个 token 的上下文长度时。这种情况与我们关注的低批处理边缘 LLM 推理场景不太相关，在该场景中，权重确实主导了总内存访问。

---

# B. 量化基础

最流行的量化方案之一是 **整数量化** ，其中浮点值被缩放并四舍五入到低精度整数。有两种广泛使用的量化模式——**对称**和 **非对称** 。对称整数量化可以表达如下：

$$
\Delta = \frac {W _ {f \max}}{2 ^ {b - 1} - 1}; W _ {q} = \text {R o u n d} \left(\frac {W _ {f}}{\Delta}\right); W _ {q f} = W _ {q} \cdot \Delta \tag {1}
$$

其中 **$W_{f}$** 是原始浮点张量，**$W_{f\max}$** 是绝对最大值，**$b$** 是量化的整数精度，**$\Delta$** 是缩放因子，**$W_{q}$** 是量化的整数值，以及 **$W_{qf}$** 是执行反量化（即重新缩放）后的浮点值。

对称量化假设张量的最小值和最大值具有相同的绝对值（即，对称值范围），但这并非总是成立。因此，另一种流行的量化模式是 **非对称量化** ，可以表达如下：

$$
\Delta = \frac {\operatorname {R a n g e} \left(W _ {f}\right)}{2 ^ {b} - 1}; z = \operatorname {R o u n d} \left(\frac {- W _ {f \min}}{\Delta}\right)
$$

$$
W _ {q} = \operatorname {R o u n d} \left(\frac {W _ {f}}{\Delta}\right) + z; \quad W _ {q f} = \left(W _ {q} - z\right) \cdot \Delta \tag {2}
$$

其中 **$W_{f\min}$** 是 **$W_{f}$** 的绝对最小值，**$z$** 表示量化张量的零点。

---

# C. 动机

我们分析了最近量化研究中广泛采用的几种技术，这些技术激发了我们提出的 BitMoD 框架。我们的讨论主要集中在权重量化上。

**量化粒度很重要。** 考虑一个浮点权重张量 **$W_{f}^{K \times D}$**，其中 **$K$** 表示输出通道数，**$D$** 是通道大小。量化模型权重有三种粒度： **每张量** （per-tensor）、 **每通道** （per-channel）和 **每组** （per-group）。每张量量化使用相同的缩放因子来量化整个权重张量，而每通道量化沿输出通道将权重张量划分为 **$K$** 个向量，并独立量化每个向量 **$W_{f}^{1 \times D}$**。然而，考虑到 LLMs 巨大的张量大小和隐藏维度，这两种粒度仍然会导致较大的量化误差。具体来说，公式 1 中反量化权重的量化误差可以表示为：
$$
\operatorname {E r r o r} \left(W _ {q f}\right) = \operatorname {E r r o r R o u n d} \left(\frac {W _ {f}}{\Delta}\right) \cdot \Delta \tag {3}
$$

其中 ErrorRound 是量化过程中的舍入误差，已表明其期望值为 0.25 [30]。因此，量化误差与缩放因子 **$\Delta$** 成正比，而 **$\Delta$** 又分别与对称（公式 1）和非对称（公式 2）量化的最大值和范围成正比。

![img](./assets/448c5297393ad234380359c4286aea6333c6b24677c17c2360af87086a30d713-1764777345033-3.jpg)

Fig. 2: 不同量化粒度的最大值和值范围。结果根据相应粒度下权重向量的标准差 **(\sigma)** 进行归一化，然后对所有权重向量求平均。每组粒度的组大小为 128。

为了进一步减少量化误差，最近的 LLM 量化研究采用了**每组粒度** [16], [19], [20], [30]。每组量化将权重通道 **$W_{f}^{1\times D}$** 进一步划分为 **$D / G$** 组，每组的大小为 **$G$**。组大小会引入额外的开销来存储每组的量化参数，即缩放因子（和零点），并且在 SOTA 量化框架中通常设置为 128 以平衡精度和内存开销 [20], [30]。图 2 展示了每组量化的优势，显示了四个代表性 LLMs 在不同粒度下的最大值和范围。 **每组粒度具有最低的最大值和范围** ，因此与

表 I 相比，将具有更低的量化误差。因此，我们在这项工作中专注于 **每组量化** 。

困惑度一般来说是用来评价`语言模型`好坏的指标。语言模型是衡量句子好坏的模型，本质上是计算句子的概率：困惑度与测试集上的句子概率相关，其基本思想是：给测试集的句子赋予较高概率值的语言模型较好,当语言模型训练完之后，测试集中的句子都是正常的句子，那么训练好的模型就是在测试集上的概率越高越好

![image-20251203235905872](./assets/image-20251203235905872.png)

表 I: Wikitext-2 困惑度 **$(\downarrow)$** 在不同量化粒度和 4 位数据类型下的比较。“PC”和“PG”分别代表每通道和每组。组大小为 128。

<table><tr><td>模型</td><td colspan="2">OPT-1.3B</td><td colspan="2">Phi-2B</td><td colspan="2">Llama-2-7B</td><td colspan="2">Llama-2-13B</td></tr><tr><td>粒度</td><td>PC</td><td>PG</td><td>PC</td><td>PG</td><td>PC</td><td>PG</td><td>PC</td><td>PG</td></tr><tr><td>FP16</td><td>14.62</td><td>14.62</td><td>9.71</td><td>9.71</td><td>5.47</td><td>5.47</td><td>4.88</td><td>4.88</td></tr><tr><td>INT4-Sym</td><td>36.05</td><td>16.04</td><td>13.03</td><td>11.15</td><td>12.92</td><td>5.84</td><td>5.47</td><td>5.07</td></tr><tr><td>INT4-Asym</td><td>48.41</td><td>15.41</td><td>12.08</td><td>10.67</td><td>8.89</td><td>5.77</td><td>5.27</td><td>5.01</td></tr><tr><td>FP4</td><td>16.07</td><td>14.99</td><td>11.24</td><td>10.68</td><td>8.07</td><td>5.77</td><td>5.15</td><td>5.05</td></tr><tr><td>Flint</td><td>15.87</td><td>16.23</td><td>11.71</td><td>11.23</td><td>6.67</td><td>6.09</td><td>5.31</td><td>5.29</td></tr></table>

**量化数据类型很重要。** 许多研究提出了用于每通道粒度量化的自定义数据类型 [25], [26], [40]。我们分析了采用不同数据类型进行每通道和每组权重量化的效果。我们探索了 4 位精度下的四种基本数据类型：对称 (INT4-Sym) 和非对称 (INT4-Asym) 整数量化、浮点 (FP4) 以及 ANT [26] 提出的 Flint 数据类型。表 I 显示了在 Wikitext-2 数据集 [33] 上的困惑度结果。我们强调两个重要的观察结果。首先，**尽管 Flint 在每通道粒度上可以获得更好的困惑度，但在每组粒度上它从未优于其他数据类型。**其次， **每组 INT4-Asym 和 FP4 量化在部分（但非全部）所研究的 LLMs 上获得了最好的困惑度** ，这表明**非对称性**和 **FP** 数据类型都有利于每组量化。这背后的原因有两方面。首先，权重张量通常表现出 **类似高斯分布** ，这与浮点数据类型非常吻合 [17], [51]。其次，虽然每组量化减轻了异常值的影响，但如先前研究所强调的 [15], [16]，权重组仍可能包含**非对称模式**的异常值，即仅为正或仅为负。这个特性受益于非对称量化。**非对称化的浮点格式？**

上述观察激励我们探索新的量化数据类型，这些数据类型可以结合非对称性和 FP 格式的优点，以在每组量化下实现更好的精度。我们注意到，基本 FP 数据类型具有对称的量化值，这是由于其固有的**符号-量级**二进制表示，其中包含正零和负零值。我们的关键见解是，我们可以通过**用另一个特殊值重新利用 冗余的零值** ，为 FP 引入额外的非对称性。这种方法为我们带来了两个主要优势。首先，它允许我们充分利用有限的量化级别。尽管冗余的零值不影响 FP16 等高精度格式，但在低精度下（例如，3 位精度下为 **$12.5\%$**），它构成了量化级别的很大一部分。其次，我们可以调整特殊值，使扩展的 FP 数据类型更好地适应每组权重分布，我们将在第 III-B 节中讨论。

**量化位宽很重要。** 尽管现有的 LLM 加速器主要依赖于支持 8 位和 4 位精度的位并行架构 [25]–[27]，但最近的研究表明，6 位浮点权重在各种 LLM 模型和任务中表现出**可忽略的精度损失** [49], [51]。受此启发，我们分析了使用不同 6 位数据类型进行每组 LLM 权重量化的效果。我们考虑了四种数据类型：对称 (INT6-Sym) 和非对称 (INT6-Asym) 整数量化，具有 2 位指数和 3 位尾数 (FP6-E2M3) 的浮点，以及具有 3 位指数和 2 位尾数 (FP6-E3M2) 的浮点。表 II 比较了不同量化数据类型在 Wikitext-2 [33] 和 C4 [18] 数据集上的困惑度结果。平均而言，所研究的 6 位数据类型与 FP16 基准相比，实现了相似且可忽略的困惑度损失。例如，INT6-Sym 的平均困惑度损失小于 0.05，其简单的整数表示为高效 LLM 加速提供了一个有前景的解决方案。因此，加速器支持**多样化的量化位宽**对于在内存占用和模型精度之间提供更好的权衡至关重要。

表 II: Wikitext-2 和 C4 困惑度 **$(\downarrow)$** 在不同 6 位数据类型下的比较。我们使用每组权重量化，组大小为 128。

<table><tr><td>模型</td><td colspan="2">OPT-1.3B</td><td colspan="2">Phi-2B</td><td colspan="2">Llama-2-7B</td><td colspan="2">Llama-2-13B</td></tr><tr><td>数据集</td><td>Wiki</td><td>C4</td><td>Wiki</td><td>C4</td><td>Wiki</td><td>C4</td><td>Wiki</td><td>C4</td></tr><tr><td>FP16</td><td>14.62</td><td>14.72</td><td>9.71</td><td>12.74</td><td>5.47</td><td>6.97</td><td>4.88</td><td>6.47</td></tr><tr><td>INT6-Sym</td><td>14.51</td><td>14.80</td><td>9.85</td><td>12.82</td><td>5.49</td><td>6.99</td><td>4.89</td><td>6.46</td></tr><tr><td>INT6-Asym</td><td>14.61</td><td>14.78</td><td>9.76</td><td>12.8</td><td>5.49</td><td>6.99</td><td>4.89</td><td>6.46</td></tr><tr><td>FP6-E2M3</td><td>14.59</td><td>14.76</td><td>9.85</td><td>12.8</td><td>5.52</td><td>6.99</td><td>4.92</td><td>6.49</td></tr><tr><td>FP6-E3M2</td><td>14.81</td><td>14.81</td><td>9.81</td><td>12.87</td><td>5.49</td><td>7.02</td><td>4.89</td><td>6.50</td></tr></table>

适应可变精度的自然解决方案是采用**位串行架构** [3], [12], [28], [45]。然而，现有的位串行加速器主要针对整数数据类型，这在 3 位精度下会导致显著的精度损失，正如我们将在第 V-B 节中所示。此外，这些加速器无法利用每组量化来提高精度。这是因为每组量化为不同的组分配了不同的缩放因子，这需要一个具有大面积开销的浮点单元来在计算每组点积后动态地反量化部分和。因此，需要一种 **低硬件成本的高效反量化机制** 。

**算法-硬件协同设计很重要。** 已经提出了许多框架来加速 LLM 执行，如表 III 所示。像 AWQ [30] 这样的 SOTA 算法解决方案将 LLM 权重量化到低精度整数，同时保持高精度。然而，AWQ 针对 GPU 上的 LLM 加速进行了优化，但 GPU 缺乏专用的混合精度计算单元。因此，它将低精度权重转换为 FP16，并依赖 GPU 浮点流水线进行计算，导致 **计算效率低下** 。

表 III: BitMoD 与 SOTA 量化 LLM 加速协同设计框架的比较。

<table><tr><td>框架</td><td>支持每组量化?</td><td>支持精度</td><td>3 位权重精度</td><td>硬件效率</td></tr><tr><td>AWQ [30]</td><td>是</td><td>有限</td><td>高</td><td>低</td></tr><tr><td>FIGNA [27]</td><td>否</td><td>有限</td><td>低</td><td>高</td></tr><tr><td>ANT [26]</td><td>否</td><td>有限</td><td>低</td><td>高</td></tr><tr><td>OliVe [25]</td><td>否</td><td>有限</td><td>中</td><td>高</td></tr><tr><td>Microscaling [40]</td><td>是</td><td>多样</td><td>低</td><td>中</td></tr><tr><td>BitMoD (本文)</td><td>是</td><td>多样</td><td>高</td><td>高</td></tr></table>

相比之下，ANT [26]、OliVe [25] 和 FIGNA [27] 提出了用于量化模型加速的 **高效位并行加速器** 。但它们的精度受限于 8 位和 4 位，这限制了利用其他精度（例如 6 位）来获得更好精度-效率权衡的能力。此外，它们的加速器不原生支持每组量化，这需要一个浮点单元来动态地反量化每组的部分和。虽然 Microscaling [40] 适应了不同的精度，但它需要一个浮点流水线来处理权重组的共享微指数，导致比其他低精度计算单元 **更高的能耗** 。此外，考虑到 LLMs 巨大的内存占用，探索**低于 4 位的量化**并保持良好的模型精度是可取的，这是 ANT、OliVe 和 Microscaling 没有解决的问题。正如我们将在第 V-B 节中所示，ANT、OliVe 和 Microscaling 提出的自定义量化数据类型在 4 位权重精度下未能实现比简单的非对称整数量化更好的精度，并在每组量化下，在 3 位权重精度下导致 **不可接受的精度损失** 。上述局限性促使我们提出一种高效的 LLM 加速框架，该框架支持广泛的硬件友好位宽，同时在低权重精度下保持良好的精度。

# III. BITMOD 量化框架

在本节中，我们将介绍  **BitMoD 量化框架** ，其中包括针对 3 位和 4 位精度的**每组量化**量身定制的新数据类型家族。第 III-A 节描述了我们提出的扩展基本 3 位和 4 位精度浮点数据类型。第 III-B 节介绍了使用所提出数据类型的增强型每组 LLM 量化策略。第 III-C 节描述了使用**整数量化因子**的硬件高效每组反量化机制。

## A. 非对称 FP3 和 FP4 数据类型

基本浮点格式由于其 **符号-量级表示** （包含 **$+0$** 和 **$-0$**），包含一个 **冗余的量化级别** 。我们建议用另一个**特殊值**替换这个冗余的零，以充分利用可用的量化级别并引入额外的 **非对称性** 。我们首先使用基本的 FP3 格式来推导出我们自定义的 3 位数据类型，然后将我们的想法扩展到 4 位精度。

**FP3 扩展。** 基本的 FP3 数据类型包含七个不同的值 **$\{0, \pm 1, \pm 2, \pm 4\}$**。我们的主要思想是扩展 FP3，并允许用预定义的一些特殊值之一替换冗余的零。因此，可以通过基本的 FP3 数据类型和一个选定的特殊值来量化一个权重组，以最小化量化误差。理想情况下，特殊值可以具有任意精度。但高精度（例如 FP16）的特殊值会导致更高的计算硬件开销，从而抵消低精度数据类型的效率优势。

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-01/f8d4377d-a546-44e7-bc70-e1a41e4a8164/bc1ded63bb6a7dd6f97f29c0864945b1770c8404eea28b632f0b7faa48c69a6d.jpg)

Fig. 3: FP3 不同特殊值（SV）下的归一化权重量化误差 **$(\downarrow)$**。我们使用组大小为 128 的每组量化。特殊值 **$\pm 6$** 实现了最低的总体量化误差，因此被 BitMoD 采用。

因此，我们将特殊值限制为 **低精度整数** 。此外，假设 **$N$** 是允许的特殊值数量，则需要 **$\lceil \log N \rceil$** 位的编码开销来指定在计算期间应选择哪个特殊值。这种选择还需要在硬件实现中使用一个 **$N$**-to-1 多路选择器。为了平衡编码开销和硬件复杂性，我们将 **$N = 4$**，这只需要每组 2 位的编码。

特殊值的选择将影响最终的量化误差，因为它改变了可用量化值的集合。正如第 II-C 节所讨论的，非对称性和浮点数据类型对于每组量化下的良好精度至关重要。由于缩放因子和量化值最终取决于数据类型的绝对最大值 [32]，我们基于两个原则建立特殊值集合。首先，一些特殊值应落在 FP3 的数值范围 **内** ，以确保它们不会改变其原始绝对最大值（即 4）。这有利于量化表现出对称、类似高斯分布的权重组。其次，一些特殊值可以落在 FP3 的数值范围 **外** ，以引入额外的非对称性，即扩展 FP3 的绝对最大值和最小值量化值不同。这有利于表现出非对称分布的权重组。

为了满足第一个属性，特殊值应设置为 **$\pm 3$**，分别用 **$+3$** 和 **$-3$** 替换冗余的零。我们将这种新数据类型称为  **FP3-ER** （Extended Resolution，扩展分辨率），因为它在 FP3 范围内增加了额外的分辨率。为了满足第二个属性，可以有无数个值落在 FP3 范围之外。因此，我们确定其余两个可以最小化量化误差的特殊值。我们通过限制这两个特殊值具有相同的绝对值来进一步缩小搜索空间，这

表 IV: 我们提出的扩展分辨率（ER）和扩展非对称性（EA）FP3 和 FP4 数据类型。

<table><tr><td>类型</td><td>基本值</td><td>扩展类型</td><td>特殊值</td></tr><tr><td rowspan="2">FP3</td><td rowspan="2">0, ±1, ±2, ±4</td><td>FP3-ER</td><td>-3 或 +3</td></tr><tr><td>FP3-EA</td><td>-6 或 +6</td></tr><tr><td rowspan="2">FP4</td><td rowspan="2">0, ±0.5, ±1, ±1.5 ±2, ±3, ±4, ±6</td><td>FP4-ER</td><td>-5 或 +5</td></tr><tr><td>FP4-EA</td><td>-8 或 +8</td></tr></table>

```
Algorithm 1: 细粒度数据类型自适应。

输入: 权重组: W; 量化精度: p

输出: 量化权重组: $W_{qout}$ ; 选定特殊值: $v_{out}$

1 Func AdaptiveQuant(W, p):

// 根据表 IV 获取基本和特殊量化值

basicValues = GetBasicValues(p)

specialValues = GetSpecialValues(p)

// 搜索最佳特殊值

minError = +∞

for v in specialValues do

    quantValues = basicValues$\cup$ v

    Wq = NonLinearQuantize(W, quantValues)

    newError = MeanSquareError(W, Wq)

if newError < minError then

    minError = newError

    Wqout = Wq

    $v_{out} = v$

return Wqout, $v_{out}$
```

导致所有权重组之间实现平衡的非对称性。这是可取的，因为尽管单个权重组可能更倾向于非对称量化，但整个权重张量通常表现出对称的、类似高斯分布 [26], [55]。图 3 显示了当向 FP3 添加不同特殊值时，六个 LLMs 上的归一化每组量化误差。我们观察到， **添加非对称性显著减少了量化误差** 。此外，特殊值 **$\pm 6$** 在大多数 LLMs 上（除了 OPT-1.3B）具有最低的量化误差，因此被 BitMoD 采用。我们将由此产生的新数据类型称为  **FP3-EA** （Extended Asymmetry，扩展非对称性），因为它增加了额外的非对称性以扩展 FP3 的范围。

**FP4 扩展。** 类似于 FP3-ER 和 FP3-EA，我们向 FP4 添加了额外的分辨率和非对称性。我们进行了实验来测量不同 FP4 特殊值对最终量化误差的影响，从而得出最佳的 FP4-ER 和 FP4-EA，它们的特殊值分别为 **$\pm 5$** 和 **$\pm 8$**。表 IV 总结了扩展的 FP3 和 FP4 数据类型。请注意，尽管我们已固定了这四个特殊值，因为它们可以最小化我们评估的各种 LLMs 的量化误差，但所提出的 BitMoD 加速器可以灵活地适应其他可能在不同 LLMs 上表现良好的任意特殊值，我们将在第 IV-A 节中讨论。

## B. 细粒度数据类型自适应

扩展的 FP3 和 FP4 数据类型包含四个特殊值，但每个权重组除了基本值外只能使用一个特殊值进行量化。因此，我们提出了一种**细粒度数据类型自适应**策略，即每个组使用不同的特殊值进行量化，以最小化量化误差，如算法 1 所详述。首先，从表 IV 获取基本值和特殊值（第 2 - 3 行）。

表 V: 使用不同精度每组缩放因子 (SF) 的 Wikitext-2 和 C4 困惑度 **$(\downarrow)$**。我们使用 INT4-Asym 进行权重量化，组大小为 128。

<table><tr><td>模型</td><td colspan="2">OPT-1.3B</td><td colspan="2">Phi-2B</td><td colspan="2">Llama-2-7B</td><td colspan="2">Llama-2-13B</td></tr><tr><td>SF 位宽</td><td>Wiki</td><td>C4</td><td>Wiki</td><td>C4</td><td>Wiki</td><td>C4</td><td>Wiki</td><td>C4</td></tr><tr><td>FP16</td><td>15.41</td><td>15.84</td><td>10.68</td><td>13.66</td><td>5.77</td><td>7.31</td><td>5.01</td><td>6.62</td></tr><tr><td>INT8</td><td>15.41</td><td>15.84</td><td>10.68</td><td>13.66</td><td>5.77</td><td>7.31</td><td>5.01</td><td>6.62</td></tr><tr><td>INT6</td><td>15.43</td><td>15.84</td><td>10.74</td><td>13.71</td><td>5.77</td><td>7.31</td><td>5.01</td><td>6.62</td></tr><tr><td>INT4</td><td>15.52</td><td>15.93</td><td>10.76</td><td>13.73</td><td>5.77</td><td>7.36</td><td>5.03</td><td>6.64</td></tr><tr><td>INT2</td><td>18.46</td><td>18.61</td><td>15.68</td><td>18.32</td><td>8.41</td><td>10.63</td><td>6.19</td><td>8.12</td></tr></table>

我们遍历所有特殊值，并在每次迭代中向基本值集合添加一个特殊值（第 5 - 6 行）。然后我们执行 **非线性量化** （第 7 行），这在以前的 PTQ 研究中常用，它将浮点张量映射到一组非线性值（即非 INT 数据类型）[19], [25], [26]。最后，我们分配在原始权重和量化权重之间具有最低均方误差的特殊值（第 8 - 11 行）。

尽管算法 1 描述了单个权重组的量化过程，但该算法可以在 GPU 上进行矢量化，以同时找到权重张量所有组的最佳特殊值。对于我们的实现，该算法只需 **$\sim 10$** 秒即可在一块 A6000 GPU 上量化整个 Llama-2-7B 模型。因此，我们提出的量化策略展现出高压缩速度和效率。

## C. 高效的每组反量化

虽然量化允许以低精度计算点积，但在生成输出后仍需要 **反量化** （即重新缩放）。具体来说，公式 1 表明量化的低精度权重应该乘以缩放因子以获得实际的浮点权重。每通道量化只需要在生成最终输出激活后进行重新缩放。这种每通道重新缩放可以进一步融合到其他逐元素操作中，例如 LayerNorm，然后才将输出激活写回内存，从而降低数据传输成本 [30], [52]。然而，每组量化必须在计算完每组点积后立即反量化 **部分和** ，因为不同组具有不同的缩放因子。此外，由于 BitMoD 将输入激活保持在 FP16，因此组部分和也将采用 **浮点格式** 。结果，即时执行反量化需要一个浮点流水线，这可能会削弱使用低精度权重所获得的潜在硬件效率。

为了降低反量化成本，我们借鉴了先前的工作 **VS-Quant** [14]，该工作应用了 **二级量化** ，将缩放因子进一步量化为低精度整数。给定权重通道大小 **$D$** 和组大小 **$G$**，VS-Quant 对来自同一通道的 **$D / G$** 个缩放因子应用对称量化（公式 1），其中每组缩放因子的精度是一个设计参数。然而，VS-Quant 仅针对小规模神经网络，并使用小的组大小 16。目前尚不清楚量化较大组的缩放因子将如何影响 LLMs 的精度。因此，我们进行了实验以找到每组缩放因子的最佳精度。我们以 INT4-Asym量化为例，而其他数据类型显示出相同的趋势。如表 V 所示，与使用 FP16 缩放因子相比， **INT8 每组缩放因子没有精度损失** 。这是预期的，因为 INT8 甚至可以实现每通道权重量化（其数值范围比缩放因子宽得多）的零精度损失 [52], [53]。因此，BitMoD 中使用 INT8 每组缩放因子，这允许以**位串行方式**进行高效的每组反量化，如第 IV-B 节所述。

**内存开销分析。** 所提出的 BitMoD 量化只需要一个 8 位缩放因子和 2 位编码元数据来为每个组选择特殊值。鉴于像 128 这样大的组大小（这在 SOTA 纯软件 LLM 量化研究中很常用 [19], [20], [30]），每组 10 位的额外内存实际上没有开销。此外，先前纯软件 PTQ 工作主要采用非对称整数量化 [19], [20], [30]，这需要每组一个 16 位缩放因子和一个 8 位零点。因此，BitMoD 相比这些工作 **展现出更低的内存开销** 。

---

# IV. BITMOD 硬件加速器

在本节中，我们描述了 BitMoD 硬件设计，该设计利用 **位串行计算范式** ，在权重精度、模型精度和硬件效率之间提供了良好的权衡。第 IV-A 节开发了 BitMoD 支持的不同低精度数据类型的 **统一位串行表示** 。第 IV-B 节详细介绍了 BitMoD 处理单元（PE）的微架构。最后，第 IV-C 节介绍了总体加速器架构。

## A. 统一位串行表示

先前的研究表明，与使用 FP16 权重相比，每通道 INT8 权重量化没有精度损失 [27], [52], [53]。此外，如第 II-C 节所讨论的，每组 INT6 量化也具有可忽略的精度损失。因此，BitMoD 硬件的设计目标是支持 INT8、INT6，以及新的 FP4 和 FP3扩展，并将其统一在一个架构中。然而，FP3 和 FP4 的基本值使用浮点格式，与整数表示不兼容。一种简单的方法是将所有数据类型转换为 INT8，但这不能提高较低精度权重的计算效率。

为了以较低的权重精度换取更高的硬件效率，我们提出了一种 **统一的位串行表示** ，其中每个数字被分解为一系列位串行项，每个项包含四个部分： **符号** （sign）、 **指数** （exp）、 **尾数** （man）和 **位有效性** （bsig）。位串行项的值可以表示为：

$$
v _ {\text {t e r m}} = (- 1) ^ {\text {s i g n}} \cdot 2 ^ {\text {e x p}} \cdot \text {m a n} \cdot 2 ^ {\text {b s i g}} \tag {4}
$$

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-01/f8d4377d-a546-44e7-bc70-e1a41e4a8164/07ee81a6a8c4f1375cf8d8779db9e0f8e3ccc3687e9891940b22a37e00b5459c.jpg)

(a)

Bsig = 4 Bsig = 0

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-01/f8d4377d-a546-44e7-bc70-e1a41e4a8164/96ec2ae95133919130cc8665a2fce2e330578fb4917257a9d060477349e13a76.jpg)

(b)

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-01/f8d4377d-a546-44e7-bc70-e1a41e4a8164/79b29ba1a0463cf5409157d44097beadd30f49ed647bcad4e8a9a8f8625a9e7a.jpg)

Fig. 4: (a) INT8、INT6 和 (b) FP4 的统一位串行表示。每个位串行项包含四个部分：符号、指数 (exp)、尾数 (man) 和位有效性 (bsig)。

图 4 描述了 BitMoD 支持的不同数据类型的位串行表示。对于 INT8 和 INT6，我们应用 **Booth 编码** [8] 将它们的二进制字符串分别分解为四个和三个 3 位 Booth 字符串。Booth 编码已广泛用于先前的位串行加速器中以加速计算 [3], [5], [43]。每两个相邻的 Booth 字符串在位有效性上相差 2。符号、指数和尾数取决于 Booth 字符串的内容，它定义了与另一个操作数 **$x$** 相乘时所需的操作。

INT Booth 项的真值表（Truth Table of INT Booth Term）

<table><tr><td>3b 字符串</td><td>操作</td><td>符号</td><td>指数</td><td>尾数</td></tr><tr><td>000 / 111</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>001 / 010</td><td>+x</td><td>0</td><td>0</td><td>1</td></tr><tr><td>110 / 101</td><td>-x</td><td>1</td><td>0</td><td>1</td></tr><tr><td>011</td><td>+2x</td><td>0</td><td>1</td><td>1</td></tr><tr><td>100</td><td>-2x</td><td>1</td><td>1</td><td>1</td></tr></table>

对于扩展的 FP4，我们首先将其转换为 **定点值** ，采用符号-量级格式，包含 1 位符号位，4 个整数位 **$\{I_3, I_2, I_1, I_0\}$** 来处理 FP4-EA 的最大特殊值 **$\pm 8$**，以及 1 个小数位 **$\{F_0\}$** 来处理基本 FP4 值的 **$\pm 0.5$** 和 **$\pm 1.5$**。然后将定点值与冗余的负零进行比较。如果比较结果相等，则负零将被替换为特定权重组分配的 **特殊值（SV）** 。允许的四个特殊值存储在一个寄存器文件（SV_reg）中，这只需要在部署 LLM 之前进行一次编程。为了获得位串行项，我们观察到表 IV 中扩展 FP4 的所有值在转换为定点格式后最多包含两个“1”位。因此，我们使用简单的**前导一位检测器（LOD）**来分别从前四个位 **$\{I_3, I_2, I_1, I_0\}$** 和后四个位 **$\{I_2, I_1, I_0, F_0\}$** 中获取两个位串行项。最后，由于扩展的 FP3 值是 FP4 的子集，因此可以使用相同的硬件解码为两个位串行项。请注意，位串行解码器不仅限于支持表 IV 中所示的特殊值。特殊值寄存器文件可以根据需要编程其他特殊值，并且可以通过对解码器进行简单修改来最小化解码的位串行项的数量。例如，特殊值 7 可以表示为两个位串行项

 和 **$-2^{0}$**，而不是使用发射三个位串行项的前导一位检测器。

## B. BitMoD 处理单元

虽然 BitMoD 能够将权重进行低精度量化，但激活仍然保持在 FP16。为了解决这个挑战，我们提出了一个如图 5 所示的 **混合精度位串行 PE** 。在每个周期中，PE 执行**四个位串行权重项** **$(w)$** 和**四个 FP16 激活** **$(a)$** 之间的 4 路点积。步骤 1 首先对齐指数之和 **$(a_{e} + w_{e})$** 以计算**指数差** **$(\delta_{e})$**。它还生成权重项和激活之间每个乘积的符号 **$(y_{s})$**。步骤 2 执行 1 位权重尾数 **$(w_{m})$** 和 11 位激活尾数 **$(a_{m})$**（包括隐藏位）之间的 **位串行乘法** 。乘法结果由一个右移位器对齐，该移位器由指数差控制。我们在移位器结果中保留 3 个额外的位，以解释四舍五入到最近偶数 [5] 所需的舍入。然后使用**加法器树**计算尾数的位串行点积。在生成位串行点积之后，步骤 3 执行 **累加** ，首先将点积乘以权重位有效性 **$(W_{bsig})$**，然后与累加器尾数 **$(m_{ACC})$** 相加。然后对累加的尾数进行**归一化**以更新累加器指数 **$(e_{ACC})$**。由于 BitMoD 采用每组量化，因此必须**即时反量化**累加的组部分和。为了降低这个硬件成本，步骤 4 以**位串行方式**执行反量化。具体来说，累加器尾数在

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-01/f8d4377d-a546-44e7-bc70-e1a41e4a8164/0fe3da1f9b8638e0b836c9c829a85a92f3b1a93995c7883f2e54398aeca0eda8.jpg)

Fig. 5: BitMoD PE 的微架构。每个位串行权重项包含 1 位符号 **$(w_{s})$**、2 位指数 **$(w_{e})$**、1 位尾数 **$(w_{m})$** 和一个共享的位有效性 **$(w_{bsig})$**。

表 VI: 在每组权重量化下使用不同数据类型的 Wikitext-2 和 C4 困惑度 (↓)。

<table><tr><td rowspan="2">精度</td><td rowspan="2">数据类型*</td><td colspan="2">OPT-1.3B</td><td colspan="2">Phi-2B</td><td colspan="2">Yi-6B</td><td colspan="2">Llama-2-7B</td><td colspan="2">Llama-2-13B</td><td colspan="2">Llama-3-8B</td><td rowspan="2">平均 ΔPPL</td></tr><tr><td>Wiki</td><td>C4</td><td>Wiki</td><td>C4</td><td>Wiki</td><td>C4</td><td>Wiki</td><td>C4</td><td>Wiki</td><td>C4</td><td>Wiki</td><td>C4</td></tr><tr><td>16-bit</td><td>FP16</td><td>14.62</td><td>14.72</td><td>9.71</td><td>12.74</td><td>5.84</td><td>8.91</td><td>5.47</td><td>6.97</td><td>4.88</td><td>6.47</td><td>6.13</td><td>8.88</td><td>0</td></tr><tr><td rowspan="5">4-bit</td><td>ANT</td><td>16.23</td><td>16.08</td><td>11.23</td><td>14.31</td><td>6.87</td><td>10.95</td><td>6.09</td><td>7.71</td><td>5.31</td><td>6.91</td><td>7.58</td><td>11.04</td><td>1.23</td></tr><tr><td>OliVe</td><td>15.38</td><td>15.82</td><td>10.49</td><td>13.51</td><td>6.55</td><td>9.84</td><td>5.91</td><td>7.34</td><td>5.13</td><td>6.74</td><td>6.89</td><td>9.91</td><td>0.68</td></tr><tr><td>MX-FP4</td><td>15.39</td><td>15.81</td><td>10.72</td><td>13.72</td><td>6.62</td><td>10.24</td><td>5.82</td><td>7.39</td><td>5.11</td><td>6.71</td><td>7.04</td><td>10.13</td><td>0.79</td></tr><tr><td>INT4-Asym</td><td>15.41</td><td>15.74</td><td>10.67</td><td>13.65</td><td>6.32</td><td>9.69</td><td>5.77</td><td>7.31</td><td>5.01</td><td>6.62</td><td>6.84</td><td>9.79</td><td>0.62</td></tr><tr><td>BitMoD</td><td>14.89</td><td>15.29</td><td>10.48</td><td>13.53</td><td>6.23</td><td>9.58</td><td>5.72</td><td>7.26</td><td>5.01</td><td>6.61</td><td>6.73</td><td>9.66</td><td>0.48</td></tr><tr><td rowspan="5">3-bit</td><td>ANT</td><td>340.6</td><td>332.9</td><td>15.57</td><td>18.35</td><td>9.01</td><td>14.32</td><td>8.51</td><td>10.28</td><td>6.40</td><td>7.98</td><td>15.22</td><td>17.56</td><td>57.61</td></tr><tr><td>OliVe</td><td>76.79</td><td>59.63</td><td>14.93</td><td>17.76</td><td>32.42</td><td>66.02</td><td>9.13</td><td>12.04</td><td>8.69</td><td>12.43</td><td>26.76</td><td>46.39</td><td>23.14</td></tr><tr><td>MX-FP3</td><td>1E+3</td><td>771.6</td><td>17.89</td><td>20.37</td><td>15.41</td><td>21.97</td><td>8.86</td><td>11.99</td><td>7.19</td><td>9.13</td><td>23.82</td><td>31.39</td><td>152.8</td></tr><tr><td>INT3-Asym</td><td>139.4</td><td>144.9</td><td>13.92</td><td>16.79</td><td>8.66</td><td>13.33</td><td>7.08</td><td>9.29</td><td>5.64</td><td>7.35</td><td>13.26</td><td>17.80</td><td>24.34</td></tr><tr><td>BitMoD</td><td>22.67</td><td>20.47</td><td>12.91</td><td>15.69</td><td>7.66</td><td>11.98</td><td>6.55</td><td>8.36</td><td>5.50</td><td>7.18</td><td>8.96</td><td>12.82</td><td>2.94</td></tr></table>

*所有量化数据类型均使用每组量化。MX 数据类型使用组大小 32，遵循 [38] 中的标准，而其他数据类型使用组大小 128。MX 的困惑度在使用更大组大小时会下降。

每个周期乘以组缩放因子 **$(\Delta_i)$** 的一位，然后进行移位-相加以获得反量化部分和的指数 **$(e_{GRP})$** 和尾数 **$(m_{GRP})$**。

关于位串行反量化，一个担忧是它是否会比正常的点积阶段花费更多周期并导致潜在的流水线停顿。如第 III-C 节所述，每组缩放因子有 8 位，需要 8 个周期进行反量化。另一方面，即使是 BitMoD 中精度最低的数据类型 FP3，也需要两个周期来处理两个位串行项。给定 PE 点积大小为 4 和常用的组大小 128，组点积阶段需要 **$128/4 \times 2 = 64$** 个周期才能完成。因此，所提出的位串行反量化 **永远不会使计算流水线停顿** 。此外，由于 INT6 和扩展的 FP4/FP3 数据类型分别包含三个和两个位串行项，所提出的 BitMoD PE 能够分别在 3 个和 2 个周期内计算 4 个乘加运算。与正常的 FP16 乘加硬件相比，BitMoD 对 INT6 和 FP4/FP3 数据类型分别实现了 **$1.33 \times$** 和 **$2 \times$** 的吞吐量提升。事实上，正如将在第 V-C 节中评估的那样，BitMoD PE 消耗的面积比 FP16 PE  **少 **$24\%$**** ，因此在等计算面积约束下能够提供更高的吞吐量。

除了权重和激活之间的矩阵乘法之外，LLMs 还包含自注意力层，其中涉及三个激活张量（即查询、键和值）之间的两次矩阵乘法操作。鉴于 BitMoD PE 仅将一个激活张量保持在 FP16 中，其他两个激活张量需要是低精度整数。幸运的是，先前的工作已经证明，由于自注意力内部的 softmax 归一化，键和值张量非常适合量化，并且可以安全地量化到 INT8 甚至 INT4，且精度损失可忽略不计 [44], [52], [58]。因此，BitMoD 可以通过将键和值张量量化为低精度整数来适应自注意力层，并使用所提出的位串行 PE。

## C. BitMoD 加速器

图 6 显示了 BitMoD 加速器的总体架构。输入和权重缓冲区经过**分块**以提供足够的带宽供 PE 访问。**位串行项生成器**接收权重数据并将其分解为位串行项，如第 IV-A 节所讨论的。主 PE 阵列包含 **$4 \times 4$** 个以**脉动方式**连接的 **瓦片** 。每个 PE 瓦片有 8 行 **$\times 8$** 列，并采用 **输出驻留数据流** 。位串行权重项广播到整个 PE 列，而输入广播到整个 PE 行。这使得 BitMoD 可以通过**权重共享**和**输入共享**来利用数据重用。每个 PE 列包含一个**本地输出缓冲区**和一个 **累加器** ，用于累加来自 PE 的每组部分和，以获得最终的每通道输出激活。由于处理一个权重组需要许多周期，因此有足够的时间仅使用一个共享累加器来 **分摊硬件成本** ，从而排空整个 PE 列。

![](./assets/f3dc31a45ebc5820deb91d0fab197e2be4fcfb8350fd23a373af3803a164a57c.jpg)

Fig. 6: BitMoD 加速器架构。

# V. 评估

## A. 实验方法

**LLM 基准。** 为了进行评估，我们选择了六个具有不同模型尺寸的代表性 LLMs，包括 OPT-1.3B [57]、Phi-2B [36]、Yi-6B [1]、Llama-2-7B、Llama-2-13B [34] 和 Llama-3-8B [35]。我们从 HuggingFace 仓库获取预训练模型，并在 PyTorch 中实现了所提出的 BitMoD 量化框架。为了评估量化对最终模型性能的影响，我们考虑了判别式和生成式任务。对于 **判别式任务** ，我们使用 LM-Evaluation-Harness [21] 在零样本设置下评估了三个基准：HellaSwag [56]、Wino-Grande [41] 和 Piqa [7]。对于 **生成式任务** ，我们选择了 Wikitext-2 [33] 和 C4 [18] 数据集，并遵循先前量化工作 [4]、[20]、[30]、[42] 中的方法评估了困惑度。

**量化数据类型。** 我们将 BitMoD 的模型精度与四种基准量化数据类型进行比较：

* **ANT** [26]，它在**每通道粒度**上自适应地使用不同的数据类型来量化不同的张量。
* **OliVe** [25]，它引入了一种 **异常值-受害者对编码机制** ，牺牲靠近异常值（outlier）的正常值（即受害者 victim）以容纳重要的异常值。
* **Microscaling (MX)** [40]，它将 32 个低精度 FP 权重与一个额外的 8 位**共享指数**分组。

表 VII: 在每组权重量化下使用不同数据类型的判别式任务精度 **$(\uparrow)$**。

<table><tr><td rowspan="2">精度</td><td rowspan="2">数据类型</td><td colspan="3">OPT-1.3B</td><td colspan="3">Phi-2B</td><td colspan="3">Yi-6B</td><td colspan="3">Llama-2-7B</td><td colspan="3">Llama-2-13B</td><td colspan="3">Llama-3-8B</td><td rowspan="2">平均 ΔAcc</td></tr><tr><td>Hella</td><td>Wino</td><td>Piqa</td><td>Hella</td><td>Wino</td><td>Piqa</td><td>Hella</td><td>Wino</td><td>Piqa</td><td>Hella</td><td>Wino</td><td>Piqa</td><td>Hella</td><td>Wino</td><td>Piqa</td><td>Hella</td><td>Wino</td><td>Piqa</td></tr><tr><td>16-bit</td><td>FP16</td><td>53.72</td><td>59.43</td><td>72.41</td><td>73.74</td><td>75.77</td><td>79.22</td><td>74.96</td><td>70.72</td><td>78.78</td><td>75.98</td><td>69.06</td><td>79.11</td><td>79.39</td><td>72.38</td><td>80.5</td><td>79.18</td><td>72.85</td><td>80.74</td><td>0</td></tr><tr><td rowspan="2">4-bit</td><td>INT4-Asym</td><td>52.31</td><td>59.35</td><td>71.05</td><td>72.29</td><td>75.14</td><td>78.4</td><td>73.91</td><td>70.51</td><td>77.64</td><td>75.29</td><td>68.74</td><td>78.22</td><td>78.76</td><td>72.45</td><td>80.2</td><td>78.07</td><td>73.24</td><td>79.76</td><td>-0.71</td></tr><tr><td>BitMoD</td><td>53.03</td><td>59.12</td><td>71.49</td><td>72.51</td><td>77.58</td><td>79.48</td><td>73.98</td><td>70.09</td><td>78.35</td><td>75.43</td><td>68.19</td><td>78.45</td><td>78.41</td><td>72.14</td><td>80.42</td><td>78.49</td><td>73.09</td><td>79.98</td><td>-0.42</td></tr><tr><td rowspan="2">3-bit</td><td>INT3-Asym</td><td>38.98</td><td>55.01</td><td>64.25</td><td>67.75</td><td>71.74</td><td>77.48</td><td>71.3</td><td>67.32</td><td>76.71</td><td>71.87</td><td>66.46</td><td>76.66</td><td>76.58</td><td>69.61</td><td>78.94</td><td>68.56</td><td>66.61</td><td>75.03</td><td>-4.84</td></tr><tr><td>BitMoD</td><td>49.16</td><td>58.09</td><td>68.88</td><td>70.16</td><td>75.22</td><td>78.18</td><td>70.72</td><td>67.72</td><td>76.28</td><td>72.68</td><td>66.22</td><td>77.53</td><td>76.79</td><td>72.37</td><td>79.22</td><td>73.56</td><td>70.32</td><td>77.91</td><td>-2.61</td></tr></table>

* **每组非对称整数量化** ，这在先前的软件量化方法中普遍采用。

为确保与 BitMoD 进行公平比较，我们仅将 ANT、OliVe 和 MX 的数据类型应用于 LLM 权重量化，同时将激活保持在 FP16。尽管最初的 ANT 和 OliVe 框架由于缺乏专用反量化硬件而仅支持每通道量化，我们已将其算法扩展为 **每组量化** 。这使得可以纯粹基于所采用的量化数据类型来比较模型精度。虽然我们的评估主要侧重于权重量化，但第 V-E 节表明 BitMoD 可以与 SOTA 激活量化方案结合，这也有潜力减少激活内存。

除了协同设计工作之外，我们还展示了 BitMoD 可以无缝集成到现有纯软件量化优化中，以进一步减少内存占用或实现更好的模型性能。我们将 BitMoD 与三种软件量化方法相结合：

* **SmoothQuant** [52]，它针对 INT8 中的权重和激活量化。它通过将大激活值部分迁移到权重来解决大激活幅值带来的量化难度。
* **AWQ** [30]，它采用**激活感知权重量化**来保护对应于较大激活幅值的显著权重通道。
* **OmniQuant** [42]，它通过块级微调优化裁剪阈值来调节异常值权重。

为了将 BitMoD 与这些纯软件方法集成，我们用 BitMoD 的扩展 FP4 和 FP3 数据类型**替换**了它们原先使用整数数据类型的权重量化器。

**加速器基准。** 为了评估硬件性能和能效，我们将 BitMoD 与支持 FP16 模型并使用 FP16

表 VIII: 使用不同数据类型量化 Llama 权重时的 Wikitext-2 和 C4 困惑度 **$(\downarrow)$**。我们使用组大小为 128 的每组权重量化。

<table><tr><td rowspan="2">精度</td><td rowspan="2">数据类型</td><td colspan="2">Llama-2-7B</td><td colspan="2">Llama-2-13B</td><td colspan="2">Llama-3-8B</td></tr><tr><td>Wiki</td><td>C4</td><td>Wiki</td><td>C4</td><td>Wiki</td><td>C4</td></tr><tr><td rowspan="4">4-bit</td><td>FP4</td><td>5.77</td><td>7.32</td><td>5.05</td><td>6.66</td><td>6.86</td><td>9.85</td></tr><tr><td>FP4-ER</td><td>5.74</td><td>7.28</td><td>5.03</td><td>6.63</td><td>6.76</td><td>9.71</td></tr><tr><td>FP4-EA</td><td>5.81</td><td>7.30</td><td>5.08</td><td>6.65</td><td>6.83</td><td>9.79</td></tr><tr><td>BitMoD</td><td>5.72</td><td>7.26</td><td>5.01</td><td>6.61</td><td>6.73</td><td>9.66</td></tr><tr><td rowspan="4">3-bit</td><td>FP3</td><td>7.51</td><td>10.28</td><td>5.90</td><td>7.58</td><td>15.22</td><td>19.87</td></tr><tr><td>FP3-ER</td><td>7.18</td><td>9.71</td><td>5.66</td><td>7.33</td><td>13.43</td><td>17.56</td></tr><tr><td>FP3-EA</td><td>6.61</td><td>8.45</td><td>5.54</td><td>7.23</td><td>9.06</td><td>12.97</td></tr><tr><td>BitMoD</td><td>6.55</td><td>8.36</td><td>5.50</td><td>7.18</td><td>8.96</td><td>12.82</td></tr></table>

乘加 PE 而不是所提出的位串行 PE 的基准加速器进行比较。我们还将 BitMoD 与 **ANT** 和 **OliVe** 进行比较，它们设计了自定义解码器以在统一的脉动阵列中支持多种数据类型。我们评估了批处理大小为 1 且输入序列长度为 256 的 LLMs 上的性能，以适应先前工作 [30] 中的边缘用例。

**硬件实现。** 我们使用 SystemVerilog 在 RTL 级实现了 BitMoD 的加速器，并通过 RTL 仿真验证了每个组件的功能。我们使用 Synopsys Design Compiler 在 TSMC 28nm 技术中综合 BitMoD 以报告面积和功耗。对于端到端性能评估，我们实现了一个 **周期级模拟器** ，其中加速器时序和能耗参数是基于 RTL 综合结果设置的。DRAM 功耗是根据 DRAMSim3 [29] 中的 DDR4 模型计算的。所有加速器都在**等计算面积约束**下进行评估，并配备了 512 KB 激活缓冲区和 512 KB 权重缓冲区，这些缓冲区使用 CACTI [6] 进行建模。

---

## B. 不同数据类型的精度比较

**生成式任务。** 表 VI 详细介绍了在 4 位和 3 位权重精度下应用不同 PTQ 数据类型的困惑度。对于 4 位和 3 位权重量化，与 FP16 基准模型相比，BitMoD 的 **平均困惑度损失分别 **$< 0.5$** 和 **$< 3$**** 。尽管 ANT、OliVe 和 MX 能够在 4 位精度下保持可接受的困惑度，但当将权重量化到 3 位时，它们的困惑度会经历显著下降。OliVe 旨在处理每通道量化下的异常值，但其优势减弱了，因为如第 II-C 节所述，异常值的影响可以通过每组量化显著减轻。MX 采用基本的 FP4 和 FP3，但没有探索其冗余零的潜力，导致其困惑度比 INT-Asym 更差，而 INT-Asym 充分利用了可用的量化级别并支持非对称性。相反， **BitMoD 在每组粒度上始终优于非对称整数量化** ，并且在 3 位精度下优势更为明显。这表明 BitMoD 中 **非对称性和浮点数据类型的组合可以显著减少量化误差** 。

表 IX: 使用不同特殊值进行 FP3 时的 Wikitext-2 和 C4 困惑度 **$(\downarrow)$**。

<table><tr><td rowspan="2">特殊值</td><td colspan="2">OPT-1.3B</td><td colspan="2">Phi-2B</td><td colspan="2">Llama-2-7B</td><td colspan="2">Llama-3-8B</td></tr><tr><td>Wiki</td><td>C4</td><td>Wiki</td><td>C4</td><td>Wiki</td><td>C4</td><td>Wiki</td><td>C4</td></tr><tr><td>{±5, ±6}</td><td>23.39</td><td>20.12</td><td>13.02</td><td>15.84</td><td>6.61</td><td>8.48</td><td>9.09</td><td>13.81</td></tr><tr><td>{±3, ±5}</td><td>35.54</td><td>37.65</td><td>13.41</td><td>16.29</td><td>6.68</td><td>8.73</td><td>10.32</td><td>14.48</td></tr><tr><td>{±3, ±6}</td><td>22.67</td><td>20.47</td><td>12.91</td><td>15.69</td><td>6.55</td><td>8.36</td><td>8.96</td><td>12.82</td></tr></table>

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-01/f8d4377d-a546-44e7-bc70-e1a41e4a8164/818d03a66a3c8492de1a2f5e1e4b9a219f5ac985d5c20339313c681f755c8b62.jpg)

Fig. 7: 不同加速器的加速比 **$(\uparrow)$**。

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-01/f8d4377d-a546-44e7-bc70-e1a41e4a8164/eba2bb8ecd1db9fd90e5aeca6015a4a7ab34695c6688bbe905822f92a2bb833d.jpg)

Fig. 8: 不同加速器的能耗 **$(\downarrow)$**。“LL”和“LY”分别代表“无损”和“有损”。

**判别式任务。** 表 VII 比较了采用 BitMoD 和基准非对称整数量化在每组粒度下的判别式任务模型精度。 **BitMoD 实现了比非对称整数量化更好或相当的精度** 。在 4 位精度下，与基准 FP16 模型相比，BitMoD 的平均精度损失 **$< 0.5\%$**。此外，与 SOTA 软件量化方法中广泛使用的非对称整数量化相比，BitMoD 的模型精度平均提高了 **$2.2\%$**。

**BitMoD 数据类型消融实验。** 如第 III-A 节所讨论的，BitMoD 通过向 FP3 和 FP4 添加额外的分辨率和非对称性来引入新的数据类型。我们分析了这些不同数据类型对三个 Llama 模型困惑度的影响。如表 VIII 所示， **具有额外分辨率和非对称性的 BitMoD 数据类型实现了最佳困惑度** 。与基本的 FP4 数据类型相比，FP4-ER 在困惑度上的改进大于 FP4-EA。这是因为在 4 位时，基本的 FP4 仍然有足够的量化级别来量化具有非对称分布的权重组，而添加额外的分辨率可以更好地减少量化误差。相反，对于 3 位精度，FP3-EA 实现了比 FP3-ER 好得多的困惑度。鉴于 3 位时的量化级别更少，FP3-EA 引入的额外非对称性在考虑具有非对称分布的权重组时具有更大的影响。

**BitMoD 特殊值消融实验。** BitMoD PE 可以灵活地支持不同的特殊值。我们评估了 FP3 的另外两种潜在特殊值组合：**$\{\pm 3, \pm 5\}$** 和 **$\{\pm 5, \pm 6\}$**。表 IX 显示了最终的 Wikitext 和

C4 困惑度。BitMoD 中采用的特殊值，即 **$\{\pm 3, \pm 6\}$**，平均实现了最低的困惑度。特殊值组合 **$\{\pm 5, \pm 6\}$** 仅向基本 FP3 引入了额外的非对称性。然而，许多权重组可能表现出对称分布，这更倾向于具有相同绝对最大值和最小值的对称数据类型。另一方面，如第 III-A 节所述，特殊值 **$\pm 5$** 具有比 **$\pm 6$** 更高的量化误差，导致与 **$\pm 3$** 结合时困惑度更差。

---

## C. 加速器性能

对于加速器评估，我们根据最终模型精度为 BitMoD 加速器考虑了两种配置：(1)  **无损（Lossless）** ，其中权重精度为 INT6，因为其在每组量化下的精度损失接近于零。我们将此配置与基准 FP16 加速器进行比较。(2)  **有损（Lossy）** ，其中权重可以量化到 4 位用于判别式任务，3 位用于生成式任务，同时保持良好的模型性能。我们将此配置与 ANT 和 OliVe 进行比较。

**瓦片面积和功耗。** 表 X 显示了在 1 GHz 频率下，基准 FP16 加速器与 BitMoD 的 PE 瓦片面积和功耗分解。统一的位串行表示

表 X: 1 GHz 频率下基准 FP16 加速器与 BitMoD 的每瓦片面积和功耗。

<table><tr><td rowspan="2"></td><td rowspan="2">PE 数量</td><td colspan="3">面积 (μm2)</td><td colspan="3">功耗 (mW)</td></tr><tr><td>PE 阵列</td><td>编码器</td><td>总计</td><td>PE 阵列</td><td>编码器</td><td>总计</td></tr><tr><td>基准</td><td>6 × 8</td><td>95,498</td><td>-</td><td>95,498</td><td>36.96</td><td>-</td><td>36.96</td></tr><tr><td>BitMoD</td><td>8 × 8</td><td>97,090</td><td>2,419</td><td>99,509</td><td>37.5</td><td>1.86</td><td>39.36</td></tr></table>

![](https://cdn-mineru.openxlab.org.cn/result/2025-12-01/f8d4377d-a546-44e7-bc70-e1a41e4a8164/e0699c4c94a2ee3bc199b7dacae09292099c587d049e5b0fa2afae025d06f60c.jpg)

Fig. 9: Phi-2B 和 Llama-2-7B 的 Wikitext-2 困惑度-EDP 帕累托图。

使得 BitMoD 能够以低硬件成本支持不同的权重数据类型。因此， **BitMoD PE 比基准 PE 小 **$24\%$**** ，这允许在相同的计算面积内容纳更多的 BitMoD PE。此外，位串行项编码器的硬件开销很小，仅占 PE 阵列面积的 **$2.5\%$**。

**性能。** 图 7 显示了判别式和生成式任务相对于基准 FP16 加速器归一化的硬件性能。 **BitMoD 在无损和有损量化下均实现了最佳性能** 。BitMoD 的性能提升主要来自于其对不同量化数据类型的精心算法-硬件协同设计。由于判别式任务是计算密集型的，主要涉及矩阵-矩阵乘法，BitMoD PE 的更高吞吐量带来了比基准 PE 更好的性能。相比之下，OliVe 需要更复杂的 PE，硬件开销显著，以容纳异常值，这些异常值具有更宽的数值范围（例如，在 4 位精度下为 **$\{24,..., 192\}$**）。相比之下，BitMoD 数据类型具有较小的取值范围，并且可以使用所提出的统一位串行表示进行高效处理。对于内存密集型的生成式任务，BitMoD 可以将 LLM 权重量化到非常低的精度，例如 3 位，这在保持良好困惑度的同时提供了显著的内存节省。相反，ANT 和 OliVe 由于缺乏专用的反量化硬件而不原生支持每组量化，并且在使用 3 位权重精度进行每通道量化时无法保持可接受的模型质量。因此，它们必须采用更高的权重精度来弥补困惑度的显著下降。总而言之，与基准 FP16 架构相比， **无损 BitMoD 在判别式和生成式任务中分别实现了 **$1.99 \times$** 和 **$2.41 \times$** 的加速比** 。与 ANT / OliVe 相比， **有损 BitMoD 在判别式和生成式任务中分别实现了 **$1.72 \times / 1.56 \times$** 和 **$1.66 \times / 1.39 \times$** 的加速比** 。

**能耗。** 图 8 显示了不同加速器的归一化能耗分解，其中片上计算能耗包括缓冲区和核心能耗。BitMoD 的节能主要来自于**减少的权重内存占用**和 **高效的位串行 PE** 。ANT 和 OliVe 都需要比 BitMoD 更高的权重精度才能保持可接受的模型质量，从而导致更高的 DRAM 能耗。此外，基准架构使用 FP16权重，这对 LLMs 来说是矫枉过正的，因为简单的 INT6 数据类型在每组权重量化下可以实现相当的精度。总而言之， **无损 BitMoD 在不同任务上比基准架构实现了 **$2.31 \times$** 更好的能效** 。有损 BitMoD 比 ANT 和 OliVe 分别具有 **$1.48 \times$** 和 **$1.31 \times$** 更好的能效。

**精度-效率权衡。** 所提出的 BitMoD 可以在模型精度和硬件效率之间提供良好的权衡。为了证明这一点，我们分析了 Phi-2B 和 Llama-2-7B 在 Wikitext-2 上的**能耗-延迟积（EDP）与模型困惑度之间的关系。我们比较了 BitMoD 与 ANT 和 OliVe 在不同 LLM 权重精度下的性能。图 9 显示了所研究的两个 LLMs 和三个加速器的困惑度-EDP 关系。请注意，虽然没有明确显示 5 位精度，但 BitMoD 可以轻松扩展，使用其 Booth 编码器执行位串行 INT5 计算。同样，ANT 和 OliVe 引入的自定义数据类型可以根据其数据类型定义扩展到 5 位精度。如图 9 所示，左下区域表示困惑度与 EDP 之间更好的权衡。尽管 ANT 和 OliVe 提出了不同的算法-硬件协同设计方法用于 LLM 加速，但它们仅利用了每通道量化粒度，无法在极低精度下保持模型质量，并且它们缺乏统一的架构来有效地支持不同的数据类型和精度。相比之下，BitMoD 利用了为每组量化量身定制的新数据类型，并采用了高效的位串行计算范式**来支持各种数据类型。因此， **BitMoD 始终位于帕累托前沿** 。

---

## D. 与混合精度位并行架构的比较

FIGNA [27] 提出了一系列 **位并行 PE** ，用于低精度整数权重和浮点激活之间的算术运算。然而，每个 FIGNA PE 都是独立设计的，只支持一种权重精度，这无法在模型精度和硬件效率之间提供权衡。我们探讨了 FIGNA 支持混合精度整数权重的可能性。我们考虑一个基准  **FIGNA 类 PE** ，它执行 FP16 激活和 INT8 权重之间的乘加运算。我们扩展了基准 PE，使其支持一个 FP16-INT8 操作或两个 FP16-INT4 操作，即用相同的 FP16 激活乘以两个 INT4 权重。图 10 比较了不同 PE 的归一化面积和功耗。尽管 FP-INT8 PE 具有最小的面积，但 **增加对混合权重精度的支持会带来显著的硬件开销** ，并导致比传统 FP-FP PE 更高的面积和功耗

![img](https://cdn-mineru.openxlab.org.cn/result/2025-12-01/f8d4377d-a546-44e7-bc70-e1a41e4a8164/b7d54b31d128a6bab4ca949a784e75c06e67648d3a2e449e440cec49a276b5df.jpg)

![](./assets/1ebaa2d606bf3395899b8efa56c1b3c4d28da0f3bab8e8e001edfacef3011f98.jpg)

Fig. 10: BitMoD 与不同位并行 PE 的归一化面积和功耗。

表 XI: 不同量化策略的 Wikitext-2 和 C4 困惑度 **$(\downarrow)$**。我们使用组大小为 128 的每组权重量化。对于每个表格列，我们将最好的两个困惑度结果以**粗体**突出显示。

<table><tr><td rowspan="2">位宽</td><td rowspan="2">方法</td><td colspan="2">Llama-2-7B</td><td colspan="2">Llama-2-13B</td><td colspan="2">Llama-3-8B</td><td rowspan="2">平均 ΔPPL</td></tr><tr><td>Wiki</td><td>C4</td><td>Wiki</td><td>C4</td><td>Wiki</td><td>C4</td></tr><tr><td>16-bit</td><td>FP16</td><td>5.47</td><td>6.97</td><td>4.88</td><td>6.47</td><td>6.13</td><td>8.88</td><td>0</td></tr><tr><td rowspan="6">4-bit</td><td>QuaRot</td><td>5.60</td><td>7.48</td><td>5.00</td><td>6.88</td><td>6.54</td><td>10.18</td><td>0.48</td></tr><tr><td>GPTQ</td><td>5.63</td><td>7.13</td><td>4.99</td><td>6.56</td><td>6.53</td><td>9.38</td><td>0.24</td></tr><tr><td>AWQ</td><td>5.60</td><td>7.12</td><td>4.97</td><td>6.56</td><td>6.54</td><td>9.39</td><td>0.23</td></tr><tr><td>OmniQ</td><td>5.59</td><td>7.12</td><td>4.96</td><td>6.56</td><td>6.57</td><td>9.50</td><td>0.25</td></tr><tr><td>BitMoD + AWQ</td><td>5.59</td><td>7.09</td><td>4.96</td><td>6.55</td><td>6.50</td><td>9.33</td><td>0.20</td></tr><tr><td>BitMoD + OmniQ</td><td>5.57</td><td>7.07</td><td>4.95</td><td>6.55</td><td>6.45</td><td>9.30</td><td>0.18</td></tr><tr><td rowspan="6">3-bit</td><td>QuaRot</td><td>6.09</td><td>8.44</td><td>5.37</td><td>7.52</td><td>7.64</td><td>12.49</td><td>1.88</td></tr><tr><td>GPTQ</td><td>6.29</td><td>7.89</td><td>5.42</td><td>7.00</td><td>9.58</td><td>11.66</td><td>1.51</td></tr><tr><td>AWQ</td><td>6.24</td><td>7.81</td><td>5.32</td><td>6.95</td><td>8.22</td><td>11.56</td><td>1.22</td></tr><tr><td>OmniQ</td><td>6.05</td><td>7.76</td><td>5.28</td><td>6.99</td><td>8.33</td><td>12.04</td><td>1.28</td></tr><tr><td>BitMoD + AWQ</td><td>6.07</td><td>7.64</td><td>5.27</td><td>6.88</td><td>7.81</td><td>11.07</td><td>0.98</td></tr><tr><td>BitMoD + OmniQ</td><td>5.89</td><td>7.59</td><td>5.21</td><td>6.85</td><td>7.57</td><td>11.05</td><td>0.89</td></tr></table>

消耗。这是因为计算两个 FP16-INT4 操作的位并行 PE 将产生两个独立的输出，使浮点累加器和输出寄存器的成本增加一倍。相比之下，位串行 BitMoD PE 在权重精度和延迟之间进行权衡，**只需要一个累加器和输出寄存器**即可用于任何权重精度。因此，与可分解的位并行 PE 相比，BitMoD 提供了最高的灵活性来支持可变权重精度，且效率更高。

---

## E. 将 BitMoD 与其他量化方案结合

通过用 BitMoD 的扩展 FP4 和 FP3 数据类型**替换**现有纯软件量化方法中原先使用整数数据类型的权重量化器，BitMoD 可以与这些方法无缝集成。我们在 AWQ [30]、OmniQuant [42] 和 SmoothQuant [52] 上演示了这种可行性。

**与量化优化正交。** 最初的 AWQ 和 OmniQuant 采用 INT-Asym 权重量化，并进行了多项算法优化，例如权重裁剪和缩放因子搜索，从而在 4 位和 3 位权重精度下实现了 SOTA 模型性能。我们评估了在 BitMoD 数据类型基础上应用 AWQ 和 OmniQuant 优化时的模型性能。我们还与 SOTA 纯软件 LLM 量化方法进行了比较，包括 GPTQ [20] 和 QuaRot [4] 在仅权重量化下的性能。表 XI 显示了使用不同量化策略的 Llama 模型的 Wikitext 和 C4 困惑度。 **将 BitMoD 与 AWQ 和 OmniQuant 结合使用显著优于其他方法** 。例如，应用 BitMoD 数据类型使 OmniQuant 的平均困惑度损失在 4 位和 3 位精度下分别降低了 **$28\%$** 和 **$31\%$**。总而言之， **BitMoD 与 AWQ 和 OmniQuant 结合使用，在 4 位和 3 位权重精度下均实现了 **$< 1$** 的平均困惑度损失，将 LLM 权重量化的极限推向了新的最先进水平** 。值得注意的是，使用 AWQ 和 OmniQuant 不会抑制 BitMoD 加速器的功能——它们的优化只是调整了每组缩放因子，BitMoD 的位串行反量化单元支持这一点。

表 XII: 当激活保持在 FP16 或量化到 INT8 并使用 SmoothQuant (SQ8) 时的 Wikitext-2 困惑度 **$(\downarrow)$**。对于权重，我们使用组大小为 128 的每组量化。

---

# VI. 相关工作

**DNN 加速器。** 关于 DNN 加速器有大量先前的工作 [3], [5], [12], [23]–[28], [39], [40], [43], [45], [48], [50], [54], [55], [59]。这些加速器提出了专用的处理单元和数据流，以匹配 DNN 的计算特性和内存访问模式。一些加速器利用值稀疏性，借助再训练来加速小规模 DNN [23], [24], [48], [50], [59]。其他工作针对基于模型量化的低精度 DNN 加速 [25]–[27], [39], [40], [54], [55]。其中，[25], [26], [40] 引入了自定义数据类型，以更好地拟合 DNN 的值分布。另一系列工作依赖于**位串行计算**来通过较低的操作数精度扩展性能，并利用位级稀疏性来跳过无效的位操作 [3], [5], [12], [28], [43], [45]。所提出的 BitMoD 结合了量化和位串行计算的优点，以有效地在权重精度和硬件效率之间进行权衡。

**LLM 量化。** 许多算法研究提出了量化解决方案来减少 LLM 的内存占用 [4], [11], [15], [19], [20], [30], [31], [42], [49], [52]。这些工作大多依赖于 LLM 权重的非对称整数量化，同时应用其他技术来优化量化参数。所提出的 BitMoD 数据类型与这些工作中的许多是**正交**的，并且可以与不同的量化优化协同组合。

---

# VII. 结论

在本文中，我们介绍了  **BitMoD** ，这是一种用于高效 LLM 加速的**算法-硬件协同设计**方案。在算法方面，BitMoD 设计了 **新的数据类型** ，这些数据类型专为极低精度下的**每组 LLM 权重量化**而定制。通过智能地将冗余的零值重新用作一个附加数字，BitMoD 扩展了 3 位和 4 位浮点数据类型的 **分辨率或范围** 。此外，BitMoD 量化框架可以与现有纯软件量化方法无缝集成，以进一步提高模型性能。在硬件方面，BitMoD 为不同的低精度数据类型提出了 **统一的位串行表示** ，并提出了**高效的位串行 PE** 来处理量化权重和 FP16 激活。我们的评估表明，BitMoD 显著优于现有的 LLM 量化方法，将 LLM 权重量化的极限推向了 **新的最先进水平** 。与现有加速器 ANT/OliVe 相比， **BitMoD 实现了 **$1.69 \times / 1.48 \times$** 的加速比和 **$1.48 \times / 1.31 \times$** 更好的能效** ，同时能够支持多样化的权重精度，从而在模型精度和硬件效率之间提供良好的权衡。

# 致谢

本项目部分得到了 Intel 公司、国家科学基金会 (National Science Foundation) 资助号 2339084，以及工程与物理科学研究委员会 (Engineering and Physical Sciences Research Council, EPSRC) 资助号 EP/S030069/1 的支持。我们要感谢 Jordan Dotzel、Hongxiang Fan、Stylianos I. Venieris、Alexandros Kouris、Mahesh Iyer、Grace Zgheib、Sergey Gribok 以及匿名审稿人提出的建设性反馈。
